<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Machine Learning for Everyone!</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="chapter_1.html"><strong aria-hidden="true">1.</strong> Quick Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="sup.html"><strong aria-hidden="true">1.1.</strong> Supervised Learning</a></li><li class="chapter-item expanded "><a href="unsup.html"><strong aria-hidden="true">1.2.</strong> Unsupervised Learning</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_2.html"><strong aria-hidden="true">2.</strong> Basic Linear Algebra needed</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chapter_2-ref.html"><strong aria-hidden="true">2.1.</strong> References</a></li></ol></li><li class="chapter-item expanded "><a href="ml_models.html"><strong aria-hidden="true">3.</strong> ML Models</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="word2vec.html"><strong aria-hidden="true">3.1.</strong> Word2Vec</a></li></ol></li><li class="chapter-item expanded "><a href="dl.html"><strong aria-hidden="true">4.</strong> Deep Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="nn.html"><strong aria-hidden="true">4.1.</strong> Neural Network</a></li><li class="chapter-item expanded "><a href="grad_descent.html"><strong aria-hidden="true">4.2.</strong> Gradient descent</a></li><li class="chapter-item expanded "><a href="back_propagation.html"><strong aria-hidden="true">4.3.</strong> Back Propagation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="bp_calc.html"><strong aria-hidden="true">4.3.1.</strong> Calculus</a></li></ol></li><li class="chapter-item expanded "><a href="act_func.html"><strong aria-hidden="true">4.4.</strong> Activation Function</a></li><li class="chapter-item expanded "><a href="cnn.html"><strong aria-hidden="true">4.5.</strong> Convolutional Neural Networks</a></li><li class="chapter-item expanded "><a href="rnn.html"><strong aria-hidden="true">4.6.</strong> Recurrent Neural Networks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="mnist.html"><strong aria-hidden="true">4.6.1.</strong> MNIST</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="tf.html"><strong aria-hidden="true">5.</strong> Tensorflow</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tfjs.html"><strong aria-hidden="true">5.1.</strong> TensorFlow.js</a></li></ol></li><li class="chapter-item expanded "><a href="torch.html"><strong aria-hidden="true">6.</strong> PyTorch</a></li><li class="chapter-item expanded "><a href="chapter_7.html"><strong aria-hidden="true">7.</strong> Transformers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="bert.html"><strong aria-hidden="true">7.1.</strong> BERT</a></li><li class="chapter-item expanded "><a href="GPT.html"><strong aria-hidden="true">7.2.</strong> GPT</a></li><li class="chapter-item expanded "><a href="T5.html"><strong aria-hidden="true">7.3.</strong> T5</a></li><li class="chapter-item expanded "><a href="copilot.html"><strong aria-hidden="true">7.4.</strong> GitHub Copilot</a></li><li class="chapter-item expanded "><a href="chapter_7_ref.html"><strong aria-hidden="true">7.5.</strong> References</a></li></ol></li><li class="chapter-item expanded "><a href="sf_e.html"><strong aria-hidden="true">8.</strong> Salesforce Einstein</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="sf_ml.html"><strong aria-hidden="true">8.1.</strong> Machine Learning</a></li><li class="chapter-item expanded "><a href="sf_nlp.html"><strong aria-hidden="true">8.2.</strong> Natural Language Processing</a></li><li class="chapter-item expanded "><a href="sf_cv.html"><strong aria-hidden="true">8.3.</strong> Computer Vision</a></li></ol></li><li class="chapter-item expanded "><a href="gcp.html"><strong aria-hidden="true">9.</strong> Google Cloud Platform</a></li><li class="chapter-item expanded "><a href="pu.html"><strong aria-hidden="true">10.</strong> Processing Units</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="cpu.html"><strong aria-hidden="true">10.1.</strong> CPU</a></li><li class="chapter-item expanded "><a href="gpu.html"><strong aria-hidden="true">10.2.</strong> GPU</a></li><li class="chapter-item expanded "><a href="tpu.html"><strong aria-hidden="true">10.3.</strong> TPU</a></li></ol></li><li class="chapter-item expanded "><a href="ml_pl.html"><strong aria-hidden="true">11.</strong> ML Pipelines</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ml-ops.html"><strong aria-hidden="true">11.1.</strong> ML ops</a></li><li class="chapter-item expanded "><a href="tfs.html"><strong aria-hidden="true">11.2.</strong> TensorFlow Serving</a></li><li class="chapter-item expanded "><a href="tfx.html"><strong aria-hidden="true">11.3.</strong> TensorFlow Extended</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="apache_airflow.html"><strong aria-hidden="true">11.3.1.</strong> Apache Airflow</a></li><li class="chapter-item expanded "><a href="apache_beam.html"><strong aria-hidden="true">11.3.2.</strong> Apache Beam</a></li><li class="chapter-item expanded "><a href="kubeflow.html"><strong aria-hidden="true">11.3.3.</strong> Kubeflow</a></li></ol></li><li class="chapter-item expanded "><a href="automl.html"><strong aria-hidden="true">11.4.</strong> AutoML</a></li></ol></li><li class="chapter-item expanded "><a href="speedup.html"><strong aria-hidden="true">12.</strong> Speedup</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="jax.html"><strong aria-hidden="true">12.1.</strong> JAX</a></li><li class="chapter-item expanded "><a href="closure.html"><strong aria-hidden="true">12.2.</strong> Closures and Decorators</a></li><li class="chapter-item expanded "><a href="jax_ref.html"><strong aria-hidden="true">12.3.</strong> References</a></li></ol></li><li class="chapter-item expanded "><a href="openai.html"><strong aria-hidden="true">13.</strong> OpenAI</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="openai-api.html"><strong aria-hidden="true">13.1.</strong> API</a></li><li class="chapter-item expanded "><a href="openai-chat.html"><strong aria-hidden="true">13.2.</strong> Chat</a></li><li class="chapter-item expanded "><a href="openai-summarize.html"><strong aria-hidden="true">13.3.</strong> Summarize</a></li><li class="chapter-item expanded "><a href="openai-tldr.html"><strong aria-hidden="true">13.4.</strong> TLDR</a></li><li class="chapter-item expanded "><a href="open-translate.html"><strong aria-hidden="true">13.5.</strong> Translate</a></li><li class="chapter-item expanded "><a href="openai-codex.html"><strong aria-hidden="true">13.6.</strong> Codex</a></li></ol></li><li class="chapter-item expanded "><a href="inspire.html"><strong aria-hidden="true">14.</strong> Inspirations</a></li><li class="chapter-item expanded "><a href="datasets.html"><strong aria-hidden="true">15.</strong> Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="datasets/boston.html"><strong aria-hidden="true">15.1.</strong> Boston Housing</a></li></ol></li><li class="chapter-item expanded "><a href="industry/ml-industry.html"><strong aria-hidden="true">16.</strong> Building ML for Industries</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="industry/lfitem-magmt.html"><strong aria-hidden="true">16.1.</strong> Lost-Found Item Management</a></li></ol></li><li class="chapter-item expanded "><a href="hardware/hardware.html"><strong aria-hidden="true">17.</strong> 17.Hardware</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="hardware/raspi.html"><strong aria-hidden="true">17.1.</strong> Raspberry Pi</a></li></ol></li><li class="chapter-item expanded "><a href="tools.html"><strong aria-hidden="true">18.</strong> Tools</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Machine Learning for Everyone!</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="chapter-1--quick-introduction"><a class="header" href="#chapter-1--quick-introduction">Chapter 1 : Quick Introduction</a></h1>
<ul>
<li>In traditional programming we start with <strong>data</strong> and <strong>hard-coded rules</strong> to apply on the data to get <strong>answers</strong>. </li>
<li>This style of programming can't bring  answers <strong>easily</strong> for problems like:
<ul>
<li>predicting a <strong>type of a cat</strong> in the given animal picture</li>
</ul>
</li>
</ul>
<p>Assume you need to write a program find out the given animal is cat or dog. Traditional way will be something like this:</p>
<pre><code class="language-py">def detect_colors(image):
# lots of code

def detect_edges(image):
# lots of code

def analyze_shapes(image):
# lots of code

def guess_texture(image):
# lots of code

def define_animal():
# lots of code

def handle_probability():
# lots of code
</code></pre>
<p>So, we will be writing lot of <strong>hard-corded</strong> rules!</p>
<p>It would be great if write a algorithm (say <strong>Classifier</strong>) which can figure out rules for us based on the data we provided (<strong>data-driven rules</strong>), so we <strong>do not have</strong> to write those rules by hand.</p>
<p>So it is trained on the Data and rules are written based on the provided data</p>
<p>So when we provide:</p>
<ul>
<li>
<p>Input: Data (say cat's image) : <em>cat image</em></p>
</li>
<li>
<p>The program takes in the given the cat image</p>
</li>
<li>
<p>Output: Predicted animal name - <em>Persian Cat with probability 89.178%</em></p>
</li>
</ul>
<p>As shown in the demo below, the user provides a image of the cat, the application predicts type of the cat in that image with a confidence (probability) with help of a Machine Learning Model. </p>
<p>Cool?</p>
<p><img src="img/1/img-rec-1.mov.webm.gif" alt="Image Recognition - 1 " /></p>
<p>To build this kind of solution using traditional programming, we may have to write too many rules or sometimes this problem is not easily solvable by our traditional programming. Here comes our hero <strong>Machine Learning</strong> to our rescue us!</p>
<h2 id="what-is-special-about-machine-learning-"><a class="header" href="#what-is-special-about-machine-learning-">What is special about Machine Learning ?</a></h2>
<p>How long it will take to write the code based on hard-coded rules for this task:</p>
<ul>
<li><strong>Solving Rubik’s Cube with a single Robot Hand</strong> using our traditional programming? </li>
</ul>
<iframe width="720" height="480" src="https://www.youtube.com/embed/kVmp0uGtShk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="steps-in-the-ml"><a class="header" href="#steps-in-the-ml">Steps in the ML</a></h2>
<p><strong>Goal:  Create an accurate Model that answers our questions most of time</strong></p>
<h3 id="step-1---gathering-data"><a class="header" href="#step-1---gathering-data">Step-1 - Gathering Data</a></h3>
<ul>
<li>To train a ML Model we need to:
<ul>
<li>Collect data to train on</li>
</ul>
</li>
</ul>
<h3 id="step-2---data-preparation"><a class="header" href="#step-2---data-preparation">Step-2 - Data Preparation</a></h3>
<ul>
<li>Load the data and visualize it</li>
<li>Check for data errors and data imbalances</li>
<li>Split the data into 2 parts
<ul>
<li>
<ol>
<li>Training Data (80%)</li>
</ol>
</li>
<li>
<ol start="2">
<li>Testing Data (20%)</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="step-3---choosing-a-model"><a class="header" href="#step-3---choosing-a-model">Step-3 - Choosing a Model</a></h3>
<p>In our case, we can use a linear model.</p>
<h3 id="step-4---training-the-model"><a class="header" href="#step-4---training-the-model">Step-4 - Training the Model</a></h3>
<p>Model </p>
<ul>
<li>
<p>\(y = mx + b\)</p>
</li>
<li>
<p>\(x\) is the input</p>
</li>
<li>
<p>\(y\) is the output (prediction)</p>
</li>
</ul>
<p>The values we are to going to adjust the training are:</p>
<ul>
<li>
<p>\(m\) (weight) and \(b\) (bias)</p>
</li>
<li>
<p>Start the training by initializing  \(m\) (weight) and \(b\) (bias) with some random values</p>
</li>
<li>
<p>At the beginning, the Model will perform very poorly</p>
<ul>
<li>We compare the model's output \(y\) with what it should have produced (target value of y)</li>
<li>We will adjust values of \(m\) (weight) and \(b\) (bias) so that we get more accurate predictions on the next time around</li>
<li>This error correction repeats...
<ul>
<li>Each iteration updates \(m\) (weight) and \(b\) (bias) - called <em>one training step</em></li>
</ul>
</li>
<li>We will stop the training once we got the good accuracy (low error)</li>
</ul>
</li>
</ul>
<h3 id="step-5---evaluating-the-model"><a class="header" href="#step-5---evaluating-the-model">Step-5 - Evaluating the Model</a></h3>
<ul>
<li>We can check the fitness of our Model using Evaluation</li>
<li>We test our Model against the <strong>Testing Data</strong> we created in Step-2
<ul>
<li>We are testing the model against data the Model has not seen yet (simulating the real-world situation)</li>
</ul>
</li>
</ul>
<h3 id="step-6---parameter-tuning"><a class="header" href="#step-6---parameter-tuning">Step-6 - Parameter Tuning</a></h3>
<p>Parameters (AKA hyper-parameters) we can tune:</p>
<ul>
<li>How many times we run through the training dataset?</li>
<li>Learning Rate
<ul>
<li>How far we did the error correction based on the information from the previous training step</li>
</ul>
</li>
</ul>
<p>These parameters determine:</p>
<ul>
<li>Accuracy of our Model</li>
<li>How long it takes to train the Model</li>
</ul>
<p>How we initialized the Model affects the Model training time
- Random values or
- Zero values</p>
<h3 id="step-7---predicationinference"><a class="header" href="#step-7---predicationinference">Step-7 - Predication/Inference</a></h3>
<p>We can use our Model to predict the values for the given input.
Power here is we can predict the values for the given input with our Model
- not by human judgement and manual rules</p>
<h2 id="videos"><a class="header" href="#videos">Videos</a></h2>
<iframe width="720" height="480" src="https://www.youtube.com/embed/cKxRvEZd3Mw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="780" height="420" src="https://www.youtube.com/embed/nKW8Ndu7Mjw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="780" height="420" src="https://www.youtube.com/embed/h0e2HAPTGF4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="supervised-learning"><a class="header" href="#supervised-learning">Supervised Learning</a></h1>
<ul>
<li>Given a set of feature/label pairs</li>
<li>Find a model predicts the label associated with a previously unseen input</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unsupervised-learning"><a class="header" href="#unsupervised-learning">Unsupervised Learning</a></h1>
<ul>
<li>Given a set of feature vectors (without labels)</li>
<li>Group them into <strong>natural clusters</strong> or create labels for groups</li>
</ul>
<p>Here are some data on the New England Patriots and let us see how we can use clustering to create groups.</p>
<pre><code>

Features:
 Name, height, weight
 
 Labeled by type of position

Receivers: (label)
    edelman = ['edelman’, 70, 200]  &lt;---- Feature vector
    hogan = ['hogan', 73, 210]
    gronkowski = ['gronkowski', 78, 265]
    amendola = ['amendola’, 71, 190]
    bennett = ['bennett’, 78, 275]

Linemen: (label)
    cannon = ['cannon’, 77, 335]
    solder = ['solder', 80, 325]
    mason = ['mason’, 73, 310]
    thuney = ['thuney', 77, 305]
    karras = ['karras', 76, 305]

</code></pre>
<pre><code class="language-py">
# r: receiver, l: linemen
ne_fb_players = [[&quot;edelman&quot;, 70, 200, &quot;r&quot;],
                 [&quot;hogan&quot;, 73, 210, &quot;r&quot;],  
                 [&quot;gronkowski&quot;, 78, 265, &quot;r&quot;], 
                 [&quot;amendola&quot;, 71, 190, &quot;r&quot;], 
                 [&quot;bennett&quot;, 78, 275, &quot;r&quot;],

                 [&quot;cannon&quot;, 77, 335, &quot;l&quot;],
                 [&quot;solder&quot;, 80, 325, &quot;l&quot;],
                 [&quot;mason&quot;, 73, 310, &quot;l&quot;],
                 [&quot;thuney&quot;, 77, 305, &quot;l&quot;],
                 [&quot;karras&quot;, 76, 305, &quot;l&quot;]
                 
                 ]

import numpy as np
import matplotlib.pyplot as plt
nep_dataset = np.array(ne_fb_players)


plt.scatter( nep_dataset[:, 1], nep_dataset[:, 2])

plt.xlabel(&quot;Height&quot;)
plt.ylabel(&quot;Weight&quot;)

plt.grid()
plt.show()




</code></pre>
<p><img src="img/1/nep_data-1.png" alt="nep dataset plot" /></p>
<pre><code class="language-py">
X = nep_dataset[:, 1:3]
print (X)
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
kmeans.labels_

</code></pre>
<pre><code>
[['70' '200']
 ['73' '210']
 ['78' '265']
 ['71' '190']
 ['78' '275']
 ['77' '335']
 ['80' '325']
 ['73' '310']
 ['77' '305']
 ['76' '305']]
array([1, 1, 0, 1, 0, 0, 0, 0, 0, 0], dtype=int32)

</code></pre>
<p>As we the first 2 items and 4th item are in one cluster while all others in the second cluster</p>
<pre><code class="language-py">plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')

# colormap viridis: https://matplotlib.org/stable/tutorials/colors/colormaps.html
 
plt.grid()
plt.show()

</code></pre>
<p><img src="img/1/kmeans-nep-1.png" alt="KMeans with 2 clusters" /></p>
<p>If we want to group them into 3 clusters, we need to provide n_clusters=3 as shown below:</p>
<pre><code>kmeans = KMeans(n_clusters=3, random_state=0).fit(X)

</code></pre>
<p><img src="img/1/kmeans-nep-2.png" alt="KMeans with 2 clusters" /></p>
<h3 id="k-means"><a class="header" href="#k-means">K-Means</a></h3>
<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory"> - Introducing k-Means</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-2---basic-linear-algebra-needed-for-ml"><a class="header" href="#chapter-2---basic-linear-algebra-needed-for-ml">Chapter 2 - Basic Linear Algebra needed for ML</a></h1>
<p>As we know computer are comfortable in dealing with numbers and perform fast operations on those numbers to provide us the results we are interested in. But in our real world we deal with <strong>things</strong> like <em>words, sentences and images</em>. This creates an <em>impedance mismatch</em>. So obvious solution will be to solve this mismatch is by representing our <strong>things</strong> in numbers, let us call this as <strong>Data Representations</strong>.</p>
<p><strong>Linear Algebra</strong> what we learned in our high school math class comes to save us here!</p>
<p>In this chapter, we will have a friendly introduction to <strong>Linear Algebra</strong>. </p>
<p>If you did not have a chance to learn <strong>Linear Algebra</strong> in your high school, do not worry, I will try to explain in a <em>simplest possible way</em> to understand the <em>Data Representations</em> concepts so we can do Machine Learning work.</p>
<hr />
<p><em>&quot;Simplicity is the ultimate sophistication&quot;</em> - <a href="https://en.wikipedia.org/wiki/Leonardo_da_Vinci">Leonardo da Vinci</a></p>
<hr />
<h2 id="let-us-use-vectors-for-data-representations"><a class="header" href="#let-us-use-vectors-for-data-representations">Let us use Vectors for Data Representations</a></h2>
<p>Ok, what is a <strong>Vector</strong>?</p>
<ul>
<li>Vector is one Dimensional Array of numbers</li>
<li>It has magnitude (value) and direction</li>
</ul>
<p><img src="img/2/vector.png" alt="vector" /></p>
<ul>
<li>Example vector with 3 entries</li>
</ul>
<pre><code>v1 = [1, 2, 3]

</code></pre>
<p>When we say <em>n-dimensional vector space</em> we mean this space consists of all vector with <em>n</em> entries.
In our vector with 3 entries, <em>3-dimensional vector space</em> will consist of all the vectors with <em>3</em> entries.</p>
<p>Another name vector space is <strong>feature</strong>, let me explain that in few moments...</p>
<h2 id="how-you-can-draw-a-point-3d-space"><a class="header" href="#how-you-can-draw-a-point-3d-space">How you can draw a point 3D space</a></h2>
<pre><code class="language-py">from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt

# width by height here are 10 inches by  10 inches
fig = plt.figure(figsize=(10,10))

# 3d projection 
# with position (pos) of subplot as num-of-rows:1, num-of-cols:1, index-of-subplot:1
# If no positional arguments are passed, defaults to (1, 1, 1).

ax = fig.add_subplot(111, projection='3d')

# plot a point
ax.scatter(2,3,4)
plt.show()
</code></pre>
<p><img src="img/2/3d-plot.png" alt="3d plot" /></p>
<h2 id="what-is-a-feature-vector"><a class="header" href="#what-is-a-feature-vector">What is a feature Vector?</a></h2>
<p>Entries of the feature vectors represent <strong>features</strong> of the thing (object) this vector is used to represent.</p>
<p>Example:
Assume the thing (object) has 3 features: <em>Color, Heaviness and Shape</em>:</p>
<ol>
<li><strong>Color</strong> = 2
<ul>
<li>say number 1 means it is Red </li>
<li>say number 2 means it is Green</li>
<li>say number 3 means it is Blue</li>
</ul>
</li>
<li><strong>Heaviness</strong> : 2
<ul>
<li>say number 1 means it is light</li>
<li>say number 2 means it is medium</li>
<li>say number 3 means it is heavy</li>
<li>say number 4 means it is super heavy</li>
</ul>
</li>
<li><strong>Shape</strong> : 1
<ul>
<li>say number 1 means it is circle</li>
<li>say number 2 means it is rectangle</li>
<li>say number 3 means it is square</li>
<li>say number 4 means it is cube</li>
</ul>
</li>
</ol>
<p>So this object with <em>Color: Green, Heaviness: medium and Shape: circle</em> is represented (Data Representation) by this feature vector whose entries are:</p>
<pre><code> fv = [2,2,1]

</code></pre>
<p>Another example:</p>
<p>The object here is a <em>Patient</em> with:</p>
<ul>
<li>height: 64 inches,</li>
<li>weight: 131 pounds, </li>
<li>age: 23 years</li>
</ul>
<p>The <em>patient</em> vector <strong>p</strong>:</p>
<pre><code>p = [64, 131, 23]
</code></pre>
<p><img src="img/2/patient-vector.png" alt="Patient Vector" /></p>
<p>Now we understand how we can provide Data Representation using <strong>Feature Vectors</strong>.</p>
<h3 id="the-object-i-have-is-an-image-how-to-do-the-data-representations-for-this"><a class="header" href="#the-object-i-have-is-an-image-how-to-do-the-data-representations-for-this">The Object I have is an Image, how to do the Data Representations for this?</a></h3>
<ul>
<li>Black and White Images
<ul>
<li>Black: 0</li>
<li>White: 1</li>
<li>Gray: 0 to 255</li>
</ul>
</li>
</ul>
<p><img src="img/2/bw-img-2.png" alt="black and white image" /></p>
<h3 id="now-we-have-words-in-say-english-dictionary-how-to-do-the-data-representations-for-these-words"><a class="header" href="#now-we-have-words-in-say-english-dictionary-how-to-do-the-data-representations-for-these-words">Now we have words in say English dictionary, how to do the Data Representations for these words</a></h3>
<ul>
<li>
<p>Naive way:</p>
<ul>
<li>words are <strong>discrete</strong> and <strong>independent</strong> tokens</li>
<li>Build Dictionary or tokens </li>
</ul>
<pre><code class="language-py">['aardvark', ... 'king', ..., 'queen', ...]
</code></pre>
<ul>
<li>Since we need to convert words to numbers as part of our Data Representations, we can assign numbers to these words</li>
</ul>
<pre><code>[0, ...  11000, ... 12000, ...]

</code></pre>
<p>We will find out these large number are not well suited for ML. We can solve this by using a concept called <strong>one-hot-vector</strong></p>
<pre><code> [ [1,0,0,0,...], ... [0,0,0,0…1,0,0…], ... [0,0,0,,0,0…1,0,0…] ]      ]
</code></pre>
<p>These vectors have <strong>same dimensionality</strong> as the <strong>number-of-words in the dictionary</strong>.</p>
<p>Suppose the English has 100,000 word, the dimension of the one-hot-vector for each word will be 100,000. In that one-hot-vector only one entry in this vector will be <strong>1</strong> and all other entries will be <strong>0</strong>.</p>
<p><img src="img/2/one-hot-vector.png" alt="one-hot-vector" /></p>
<h4 id="disadvantages-with-one-hot-vector-way-of-data-representation"><a class="header" href="#disadvantages-with-one-hot-vector-way-of-data-representation">Disadvantages with one-hot-vector way of Data Representation</a></h4>
<ul>
<li>Very high dimensionality </li>
<li>Do not capture any world knowledge (like: Gender, Part-of-Speech...) about the words
<ul>
<li>example: <strong>king</strong> and <strong>queen</strong> are more in common with each other than <strong>aardvark</strong>
<ul>
<li>all of these token have <strong>90 degrees</strong> angles between them</li>
</ul>
</li>
</ul>
</li>
<li>Let us take a dictionary with only these 3 words</li>
</ul>
<pre><code>['aardvark', 'king', 'queen'] 

</code></pre>
<p><img src="img/2/word-rep-1.png" alt="word rep" /></p>
<ul>
<li>
<p>These 3 words in vectors in above 3-dimensional space</p>
</li>
<li>
<p>They are unit vectors aligned to axes</p>
</li>
<li>
<p>We need to find a way the words to occupy the entire this 3-dimensional space instead of perfectly aligning to the axes.</p>
</li>
<li>
<p>More useful Data Representation of words will be continuous vectors in the n-dimensional space</p>
<ul>
<li>This will allow <strong>aardvark</strong>, <strong>king</strong> and <strong>queen</strong> to flow anywhere in this 3-dimensional space. So their representations will be real-values like <strong>0.3</strong>, <strong>1.9</strong>, <strong>-0.4</strong> for <strong>aardvark</strong></li>
</ul>
<p><img src="img/2/num-rep-cont-1.png" alt="Word Continuous Representation" /></p>
</li>
<li>
<p>Representing world knowledge (like: Gender, Part-of-Speech...)</p>
</li>
<li>
<p>For example for <strong>queen</strong> [0.1, -0.3, 1.2,     -0.4, 0.02, 1.1,    -0.25, ... ]</p>
<ul>
<li>First 3 entires can represent the aspect of <strong>Gender</strong> for example</li>
<li>Next 3 entires can represent the aspect of <strong>Part-of-speech</strong> for example </li>
</ul>
</li>
<li>
<p>This mechanism will help us to express <strong>relationships between the words</strong> as equal to relative <strong>vector distance</strong></p>
</li>
</ul>
<p><img src="img/2/word-embedding-1.png" alt="word embedding" /></p>
<ul>
<li>For example, for Gender Dimension, <strong>king and queen</strong> should be far part as <strong>man and woman</strong>
<ul>
<li>In case of Part-of-speech dimensions, all these words should clustered together at a distance zero, since all of them are <strong>nouns</strong></li>
<li><strong>play</strong> (verb) and <strong>playful</strong> (adjective) should be at the same distance as <strong>joy</strong> and <strong>joyful</strong></li>
</ul>
</li>
</ul>
<h4 id="how-we-can-learn-useful-embedding-data-representations"><a class="header" href="#how-we-can-learn-useful-embedding-data-representations">How we can learn useful embedding (Data Representations)?</a></h4>
<ul>
<li>
<p><strong>Wikipedia</strong> comes to rescue us here! It is a reliable source of information we can use to learn the useful embeddings:</p>
<ul>
<li>has 28 billion words</li>
<li>309 languages</li>
</ul>
</li>
<li>
<p>If we look at the <strong>king</strong> and <strong>queen</strong> in Wikipedia, both them have lot of commonality</p>
<ul>
<li>They reference each other, includes common words like monarch</li>
<li>So Wikipedia has word knowledge we can extract to learn the useful embedding for us</li>
</ul>
</li>
<li>
<p>Wikipedia is freeform text</p>
<ul>
<li>Common practice
<ul>
<li>Unsupervised text data ---&gt; supervised task</li>
<li>we can ask the ML model to fill in the gaps and predict the next word as shown below:</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>King is the title given to male ______

monarch : 70%
body : 20%
dog: 10%

</code></pre>
<ul>
<li>The ML model could produce a probability distribution over the word in the vocabulary indicating which ones are more likely to follow the word given so far, in our case it is <strong>monarch</strong></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="21-references"><a class="header" href="#21-references">2.1. References</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/LlKAna21fLE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/fNk_zzaMoSs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<ul>
<li><a href="https://mathinsight.org/vector_introduction">An introduction to vectors</a></li>
<li><a href="https://www.youtube.com/watch?v=LE3NfEULV6k">Transfer learning and Transformer models (ML Tech Talks)</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="3-ml-models"><a class="header" href="#3-ml-models">3. ML Models</a></h1>
<h2 id="simple-model"><a class="header" href="#simple-model">Simple model</a></h2>
<p>\( y = mx + b \)</p>
<pre><code> where
 m = slope (gradient)
 b = y-intercept

x is the independent variable
y is the dependent variable depends on m and b

</code></pre>
<h3 id="plotting-the-equation"><a class="header" href="#plotting-the-equation">Plotting the equation</a></h3>
<p>\( y = x*2 + 1 \)</p>
<p><img src="img/models/eqn-plotting-0.png" alt="ML Model plotting" /></p>
<pre><code class="language-py">import matplotlib.pyplot as plt
import numpy as np

# setup the plot size 10 inches by 10 inches
fig = plt.figure(figsize=(10,10))

# 1 row, 1 col, and index is 1
ax = fig.add_subplot(111)

# put grid in the plot
plt.grid()

# let us generate x values start from -5 to 5  with 100 samples
x = np.linspace(-5,5,100)
print ('Number of samples = {}' .format(len(x)))

ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position('center')

ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')

# we need ticks at bottom and left
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

## our plot function
def plot_eqn(eqn,  color, label):
  plt.plot(x, eqn, color, label=label)
  # put legend at upper left cornor
  plt.legend(loc='upper left')

plot_eqn( x*2 + 1, '-r', 'eqn for x*2 + 1')
#plot_eqn( x*2 - 1, '-b', 'eqn for x*2 - 1')
#plot_eqn( x*2 - 3, ':b', 'eqn for x*2 - 3')
#plot_eqn( x*2 + 3, '--m', 'eqn for x*2 + 3')

## show our plot
plt.show()


</code></pre>
<h2 id="what-happens-when-we-train-a-ml-model-for-this-equation"><a class="header" href="#what-happens-when-we-train-a-ml-model-for-this-equation">What happens when we train a ML model for this equation?</a></h2>
<ul>
<li>We provide a training dataset with values for <strong>x</strong> and <strong>y</strong></li>
</ul>
<table><thead><tr><th>x</th><th>y</th></tr></thead><tbody>
<tr><td>2</td><td>5</td></tr>
<tr><td>1</td><td>3</td></tr>
<tr><td>7</td><td>15</td></tr>
<tr><td>...</td><td>...</td></tr>
</tbody></table>
<ul>
<li>During the training ML Model calculates the optimum value for <strong>m</strong> and <strong>b</strong> variables based on the training dataset we have provided</li>
<li>Once training completed, ML model is ready for predicting value for <strong>y</strong> for the given <strong>x</strong></li>
</ul>
<pre><code> You:   Hey, model my x value is 2, can you predict the value of y?
 Model: Sure, it is 5
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="word2vec"><a class="header" href="#word2vec">Word2Vec</a></h1>
<p><a href="https://en.wikipedia.org/wiki/Word2vec">Word2vec</a> is a technique for natural language processing (NLP).
Word2vec is used to produce word embeddings.</p>
<p>Uses a neural network model to learn <strong>word associations</strong> from a <strong>large corpus of text</strong>.</p>
<p>Once trained, such a model can detect <em>synonymous words</em>  or suggest additional words for a partial sentence.</p>
<p>Word2vec represents each <em>distinct word</em> with a particular list of numbers called a <em>vector</em>.</p>
<p>The vectors are chosen carefully such that a simple mathematical function (the <em>cosine similarity</em> between the vectors) indicates the level of <em>semantic similarity</em> between the words represented by those vectors.</p>
<p>Takes in large corpus of text as input and produces a vector space, typically of <em>several hundred dimensions</em>, with each unique word in the corpus being assigned a corresponding vector in the space.</p>
<ul>
<li>Word vectors are positioned in the vector space such that words that share <em>common contexts</em> in the corpus are located <em>close to one another</em> in the space.</li>
</ul>
<h3 id="papers"><a class="header" href="#papers">Papers</a></h3>
<ul>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed Representations of Words and Phrases and their Compositionality</a></li>
</ul>
<h3 id="continuous-bag-of-words-model"><a class="header" href="#continuous-bag-of-words-model">Continuous Bag-of-Words Model</a></h3>
<ul>
<li>Predicts the middle word based on surrounding context words. 
<ul>
<li>The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.</li>
</ul>
</li>
</ul>
<h3 id="continuous-skip-gram-model"><a class="header" href="#continuous-skip-gram-model">Continuous Skip-gram Model</a></h3>
<ul>
<li>Predicts words within a certain range before and after the current word in the same sentence. </li>
</ul>
<p>Consider the following sentence of 8 words:</p>
<pre><code>The wide road shimmered in the hot sun.
</code></pre>
<p>The context words for each of the 8 words of this sentence are defined by a <em>window size</em>.
The window size determines the <em>span of words on either side of a target_word</em> (one underlined) that can be considered context word. Take a look at this table of skip-grams for target words based on different window sizes.</p>
<p><img src="https://tensorflow.org/tutorials/text/images/word2vec_skipgram.png" alt="window size" /></p>
<h4 id="training-objective"><a class="header" href="#training-objective">Training Objective</a></h4>
<ul>
<li>Maximize the probability of predicting <strong>context words</strong> (w) given the target word (\(w_t\)).</li>
<li>For a sequence of words \(w_1,w_2, ... w_T\), the objective can be written as the average log probability. where \(c\) is the size of the training context. </li>
</ul>
<p><img src="https://tensorflow.org/tutorials/text/images/word2vec_skipgram_objective.png" alt="prob" /></p>
<h3 id="notebooks"><a class="header" href="#notebooks">Notebooks</a></h3>
<ul>
<li>
<p><a href="https://colab.research.google.com/drive/1oSx0wd2lC4B15KWDinOjqptZtytqRkhk?usp=sharing">word2vec</a></p>
</li>
<li>
<p><a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469#file-understanding-word-vectors-ipynb">Understanding word vectors</a></p>
</li>
<li>
<p><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb">Word2Vec - TensorFlow</a></p>
</li>
</ul>
<h3 id="videos-1"><a class="header" href="#videos-1">Videos</a></h3>
<iframe width="720" height="480" src="https://www.youtube.com/embed/LSS_bos_TPI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/L3D0JEA1Jdc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/mI23bDF0VRI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="word-embedding"><a class="header" href="#word-embedding">Word embedding</a></h2>
<p>Word embedding is a term used for the representation of words for text analysis, typically in the form of a <em>real-valued vector</em> that encodes the <em>meaning of the word</em> such that:
-  the words that are <em>closer in the vector space</em> are expected to be <em>similar</em> in meaning</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="4-deep-learning"><a class="header" href="#4-deep-learning">4. Deep Learning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="neural-network"><a class="header" href="#neural-network">Neural Network</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/aircAruvnKk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></li>
</ul>
<p><strong>Neural networks</strong> is a beautiful biologically-inspired programming <strong>paradigm</strong> which enables a computer to learn from <strong>observational data</strong>.</p>
<p><strong>Deep learning</strong>, a powerful set of techniques for <strong>learning in neural networks</strong>.</p>
<p>Provide solutions in :</p>
<ul>
<li>image recognition</li>
<li>speech recognition</li>
<li>natural language processing (NLP)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-descent"><a class="header" href="#gradient-descent">Gradient descent</a></h1>
<p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html">Gradient descent</a> is an optimization algorithm used to:</p>
<ul>
<li>minimize some function (cost function) by iteratively moving in the <strong>direction of steepest descent</strong> as defined by the negative of the gradient. </li>
</ul>
<p>In machine learning, we use gradient descent to update the parameters (weights and biases) of our model.</p>
<p><img src="https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png" alt="grad descent" /></p>
<ul>
<li>Starting at the top of the mountain, we take our first step <em>downhill</em> in the direction specified by the <em>negative gradient</em>. </li>
<li>Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. </li>
<li>We continue this process iteratively until we get to the <em>bottom of our graph</em>, or to a point where we can no longer move downhill.</li>
</ul>
<h2 id="learning-rate"><a class="header" href="#learning-rate">Learning Rate</a></h2>
<p>The size of these steps is called the <strong>learning rate</strong>.</p>
<p>With a <strong>high learning rate</strong> we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing.</p>
<p>With a <strong>very low learning rate</strong>, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.</p>
<h2 id="cost-function"><a class="header" href="#cost-function">Cost function</a></h2>
<ul>
<li>
<p>It is a loss function. </p>
</li>
<li>
<p>It is a measure of how wrong the model is in terms of its ability to estimate the relationship between x and y </p>
</li>
<li>
<p>It is a measure of how far we are away from the target:</p>
<ul>
<li>\(y - (mx + b)\)</li>
<li>Cost function :
<ul>
<li>\[ f(m,b) = \frac{1}{N} \sum_{i=0}^n (y_i - (mx_i + b))^2 \]</li>
</ul>
</li>
</ul>
</li>
<li>
<p>This tells us <strong>how bad</strong> our model is at making predictions for a given set of parameters. </p>
</li>
</ul>
<p>The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters (weight) to make the model more accurate.</p>
<p>We run gradient descent using our cost function.</p>
<ul>
<li>Calculate the partial derivatives of the cost function \( f(m,b)    \) with respect to each parameter( m and b) and store the results in a gradient.</li>
<li>This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters (m and b).</li>
</ul>
<h3 id="derivative"><a class="header" href="#derivative">Derivative</a></h3>
<p><a href="https://en.wikipedia.org/wiki/Derivative">Derivative</a> of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value).</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/0/0f/Tangent_to_a_curve.svg" alt="derivative" /></p>
<p>The slope of the tangent line is equal to the derivative of the function at the marked point.</p>
<h3 id="partial-derivative"><a class="header" href="#partial-derivative">Partial derivative</a></h3>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Partial_derivative">Partial derivative</a> of a function of several variables (in our case m and b)  is:</p>
<ul>
<li>its <a href="https://en.wikipedia.org/wiki/Derivative">derivative</a> with respect to one of those variables.</li>
</ul>
</li>
<li>
<p>with respect to m (weight): \( \frac{df}{dm}\)</p>
<ul>
<li>-2x(y - (mx + b))</li>
<li>\[  \frac{1}{N} \sum_{i=0}^n -2x_i(y_i - (mx_i + b)) \]</li>
</ul>
</li>
<li>
<p>with respect to b (bias): \( \frac{df}{db}\)</p>
<ul>
<li>-2(y - (mx + b))</li>
<li>\[  \frac{1}{N} \sum_{i=0}^n -2(y_i - (mx_i + b)) \] </li>
</ul>
</li>
</ul>
<iframe width="720" height="480" src="https://www.youtube.com/embed/HaHsqDjWMLU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/tIeHLnjs5U8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<pre><code class="language-py">

# Y is target for the given input X
# mx + b is predicted
# learning_rate is size of the steps

def update_weights(m, b, X, Y, learning_rate):
    m_deriv = 0
    b_deriv = 0
    N = len(X)
    for i in range(N):
        # Calculate partial derivatives

        # -2x(y - (mx + b))
        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))

        # -2(y - (mx + b))
        b_deriv += -2*(Y[i] - (m*X[i] + b))

    # We subtract because the derivatives point in direction of steepest ascent
    m -= (m_deriv / float(N)) * learning_rate
    b -= (b_deriv / float(N)) * learning_rate

    return m, b

</code></pre>
<iframe width="720" height="480" src="https://www.youtube.com/embed/IHZwWFHWa-w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="back-propagation"><a class="header" href="#back-propagation">Back Propagation</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/Ilg3gGewQ5U" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="calculus"><a class="header" href="#calculus">Calculus</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/tIeHLnjs5U8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="activation-function"><a class="header" href="#activation-function">Activation Function</a></h1>
<p>Activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. </p>
<p>When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be <strong>fired to the next neuron</strong></p>
<h2 id="sigmoid-function"><a class="header" href="#sigmoid-function">Sigmoid function</a></h2>
<p>A sigmoid function is a mathematical function having a characteristic &quot;S&quot;-shaped curve or sigmoid curve.</p>
<p><img src="img/1/sigmoid-1.png" alt="Sigmoid" /></p>
<ul>
<li>\( \sigma(x) =  \frac{1}{1 + e^{-x}}\)</li>
</ul>
<pre><code class="language-py">
import math
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x):
    a = []
    for item in x:
        a.append(1/(1 + math.exp(-item)))
    return a

x = np.arange(-10., 10., 0.2)
sig = sigmoid(x)

# plot sig
plt.plot(x,sig)
plt.show()

</code></pre>
<h2 id="hyperbolic-tangent-activation-function"><a class="header" href="#hyperbolic-tangent-activation-function">Hyperbolic tangent activation function</a></h2>
<p>It is also referred  the \(Tanh\) (also “tanh” and “TanH“) function. 
It is very similar to the <strong>sigmoid activation function</strong> and even has the same S-shape. 
The function takes any real value as input and outputs values in the range -1 to 1.</p>
<pre><code class="language-py"># plot for the tanh activation function
from math import exp
import matplotlib.pyplot as plt
 
# tanh activation function
def tanh(x):
	return (exp(x) - exp(-x)) / (exp(x) + exp(-x))
 
# define input data
inputs = [x for x in range(-10, 10)]
# calculate outputs
outputs = [tanh(x) for x in inputs]

# plot inputs vs outputs
plt.plot(inputs, outputs)
plt.grid()
plt.show()

</code></pre>
<p><img src="img/1/tanh-1.png" alt="TanH" /></p>
<p><a href="https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/">Refer: How to Choose an Activation Function for Deep Learnin</a></p>
<h2 id="softmax"><a class="header" href="#softmax">Softmax</a></h2>
<pre><code class="language-py">from numpy import exp

# softmax activation function
def softmax(x):
	return exp(x) / exp(x).sum()

# define input data
inputs = [1.0, 3.0, 2.0]
# calculate outputs
outputs = softmax(inputs)
# report the probabilities
print(outputs)
# report the sum of the probabilities
print(outputs.sum())

</code></pre>
<pre><code class="language-bash">[0.09003057 0.66524096 0.24472847]
1.0
</code></pre>
<h2 id="rectified-linear-activation-function"><a class="header" href="#rectified-linear-activation-function">Rectified Linear Activation Function</a></h2>
<p>A node or unit that implements this activation function is referred to as a rectified linear activation unit, or ReLU for short. </p>
<pre><code class="language-py">
# demonstrate the rectified linear function
 
# rectified linear function
def rectified(x):
	return max(0.0, x)
 
# demonstrate with a positive input
x = 1.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))
x = 1000.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))
# demonstrate with a zero input
x = 0.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))
# demonstrate with a negative input
x = -1.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))
x = -1000.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))

</code></pre>
<h3 id="plotting"><a class="header" href="#plotting">Plotting</a></h3>
<pre><code class="language-py"># plot inputs and outputs
from matplotlib import pyplot
 
# rectified linear function
def rectified(x):
	return max(0.0, x)
 
# define a series of inputs
series_in = [x for x in range(-10, 11)]
# calculate outputs for our inputs
series_out = [rectified(x) for x in series_in]
# line plot of raw inputs to rectified outputs
pyplot.plot(series_in, series_out)
pyplot.show()
</code></pre>
<p><img src="img/1/rectified-1.png" alt="Rectified" /> </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="convolutional-neural-networks"><a class="header" href="#convolutional-neural-networks">Convolutional Neural Networks</a></h1>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<ul>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_1.pdf">CNN for Visual Recognition</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_2.pdf">Image Classification</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_5.pdf">Convolutional Neural Networks - Lecture-5</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_3.pdf">Loss Functions and Optimization</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_4.pdf">Neural Networks and Backpropagation</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_5.pdf">CNN</a></p>
</li>
<li>
<p>[Hardware and Software][http://cs231n.stanford.edu/slides/2021/lecture_6.pdf]</p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_9.pdf">CNN Architectures</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_7.pdf">Convolutional Neural Networks - Lecture-7 - Training Neural Networks - Part-1s</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_8.pdf">Convolutional Neural Networks - Lecture-7 - Training Neural Networks - Part-2</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="recurrent-neural-networks"><a class="header" href="#recurrent-neural-networks">Recurrent Neural Networks</a></h1>
<h2 id="generative-models"><a class="header" href="#generative-models">Generative Models</a></h2>
<p>Given training data, generate new samples from same distribution</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_9.pdf">RNN</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_11.pdf">Attention and Transformers</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_12.pdf">Generative Models</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_13.pdf">Self-Supervised Learning</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_14.pdf">Visualizing and Understanding</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_15.pdf">Detection and Segmentation</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mnist"><a class="header" href="#mnist">MNIST</a></h1>
<p>The MNIST database <em>Modified National Institute of Standards and Technology database</em> is a large database of <strong>handwritten digits</strong> that is commonly used for training various image processing systems.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png" alt="MNIST" /></p>
<h2 id="sample-application-for-handwritten-digit-recognition"><a class="header" href="#sample-application-for-handwritten-digit-recognition">Sample Application for handwritten digit recognition</a></h2>
<iframe src='https://mohan-chinnappan-n2.github.io/2021/ai/mnist/minst.html' 
width="860" height="400">
</iframe>
<ul>
<li><a href="https://mohan-chinnappan-n2.github.io/2021/ai/mnist/minst.html">MNIST App - brain.js based</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="5-tensorflow"><a class="header" href="#5-tensorflow">5. Tensorflow</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensorflowjs"><a class="header" href="#tensorflowjs">TensorFlow.js</a></h1>
<p>TensorFlow.js (TFJS) is a library for machine learning in JavaScript.
Using TFJS you can develop ML models in JavaScript, and use ML directly in the browser or in Node.js.</p>
<h2 id="browser"><a class="header" href="#browser">Browser</a></h2>
<pre><code class="language-js">
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js&quot;&gt;&lt;/script&gt;

</code></pre>
<h2 id="nodejs"><a class="header" href="#nodejs">Node.js</a></h2>
<pre><code class="language-bash">
# install TensorFlow.js. using npm or yarn
yarn add @tensorflow/tfjs

# Install TensorFlow.js with native C++ bindings.
yarn add @tensorflow/tfjs-node

# if your system has a NVIDIA® GPU with CUDA support, use the GPU package even for higher performance.
yarn add @tensorflow/tfjs-node-gpu


</code></pre>
<pre><code class="language-js">
const tf = require('@tensorflow/tfjs');

// Optional Load the binding:
// Use '@tensorflow/tfjs-node-gpu' if running with GPU.
require('@tensorflow/tfjs-node');

// Train a simple model:
const model = tf.sequential();
model.add(tf.layers.dense({units: 100, activation: 'relu', inputShape: [10]}));
model.add(tf.layers.dense({units: 1, activation: 'linear'}));
model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});

const xs = tf.randomNormal([100, 10]);
const ys = tf.randomNormal([100, 1]);

model.fit(xs, ys, {
  epochs: 100,
  callbacks: {
    onEpochEnd: (epoch, log) =&gt; console.log(`Epoch ${epoch}: loss = ${log.loss}`)
  }
});
  
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="6-pytorch"><a class="header" href="#6-pytorch">6. PyTorch</a></h1>
<p>PyTorch is an open source machine learning library based on the <strong>Torch library</strong>, used for applications such as :</p>
<ul>
<li>computer vision </li>
<li>natural language processing
primarily developed by Facebook's AI Research lab.</li>
</ul>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<ul>
<li><a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c30c1dcf2bc20119bcda7e734ce0eb42/quickstart_tutorial.ipynb#scrollTo=M10FvMZFJPvc">Tutorial</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="7-transformers"><a class="header" href="#7-transformers">7. Transformers</a></h1>
<p>A <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer</a> is a deep learning model that adopts the mechanism of <strong>self-attention</strong>, <strong>differentially weighting the significance</strong> of each part of the input data.</p>
<p>Like recurrent neural networks (RNNs), transformers are designed to <strong>handle sequential input data</strong>, such as natural language, for tasks such as <strong>translation and text summarization</strong>. 
However, unlike RNNs, transformers do not necessarily process the data in order. Instead the attention mechanism provides <strong>context for any position</strong> in the input sequence. </p>
<p>For example, if the input data is a natural language sentence, the transformer <strong>does not need to process the beginning of the sentence before the end</strong>. Rather, it identifies the <strong>context</strong> that confers <strong>meaning</strong> to each word in the sentence. 
This feature allows for more parallelization than RNNs and therefore reduces training times.</p>
<p>Before transformers, most state-of-the-art NLP systems relied on gated RNNs, such as LSTM and gated recurrent units (GRUs), with <strong>added attention</strong> mechanisms. </p>
<p>Transformers are built <strong>on these attention</strong> technologies without using an RNN structure, highlighting the fact that <strong>attention mechanisms alone can match the performance</strong> of RNNs with attention.</p>
<p>Gated RNNs process tokens sequentially, maintaining <strong>a state vector</strong> that contains a representation of the data seen after every token.</p>
<p>To process the <em>n</em>th token, the model combines the state representing the sentence up to token <em>n-1</em> with the information of the new token to create a <strong>new state</strong>, representing the sentence up to token <em>n</em>. </p>
<p><strong>Theoretically</strong>, the information from one token can propagate arbitrarily far down the sequence, if at every point the state continues to encode contextual information about the token. </p>
<p><strong>In practice</strong> this mechanism is flawed: the <strong>vanishing gradient problem</strong> leaves the model's state at the end of a long sentence <strong>without precise, extractable information about preceding tokens</strong>.</p>
<p>This problem was addressed by <strong>attention mechanisms</strong>. Attention mechanisms let a model draw from the state at any preceding point along the sequence. </p>
<p>The attention layer can access <strong>all previous states and weigh them according to a learned measure of relevancy</strong>, providing relevant information about far-away tokens.</p>
<p>A clear example of the value of attention is in <strong>language translation</strong>, where <strong>context is essential</strong> to assign the meaning of a word in a sentence. 
In an English-to-French translation system, the first word of the French output most probably depends heavily on the <strong>first few words of the English input</strong>. 
However, in a classic LSTM model, in order to produce the first word of the French output, the model is <strong>given only the state vector of the last English word</strong>. 
Theoretically, this vector can encode information about the whole English sentence, giving the model all necessary knowledge. 
In practice, this information is often <strong>poorly preserved by the LSTM</strong>. 
An <em>attention mechanism can be added to address this problem</em>: 
- the decoder is given <em>access to the state vectors of every English input word, not just the last</em>, and can learn attention weights that dictate how much to attend to each English input state vector.</p>
<p>Transformers use an attention mechanism <strong>without an RNN</strong></p>
<ul>
<li>processing all tokens at the same time </li>
<li>calculating attention weights between them in <strong>successive layers</strong>.</li>
</ul>
<h3 id="vanishing-gradient-problem"><a class="header" href="#vanishing-gradient-problem"><a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">Vanishing Gradient problem</a></a></h3>
<p>In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with <em>gradient-based learning methods and backpropagation</em>.
- In such methods, each of the neural network's <strong>weights</strong> receives an update proportional to the <strong>partial derivative</strong> of the error function with respect to the current weight in each iteration of training.
- The problem is that in some cases, the <strong>gradient will be vanishingly small</strong>, effectively <strong>preventing the weight from changing its value</strong>
- In the worst case, this may completely <strong>stop</strong> the neural network from further training</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bert"><a class="header" href="#bert">BERT</a></h1>
<p>BERT, which stands for Bidirectional Encoder Representations from Transformers, is based on <strong>Transformers</strong>, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their <strong>connection</strong>.</p>
<p>BERT is a technology to generate <strong>contextualized</strong> word embeddings/vectors, which is its biggest advantage but also it's biggest disadvantage as it is <strong>very compute-intensive at inference time</strong>, meaning that if you want to use it in production at scale, it can become costly.</p>
<ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf">Paper: Attention Is All You Need</a></li>
</ul>
<p><strong>Transformer encoder</strong> reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word)</p>
<ul>
<li><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li><a href="https://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/bert">Hugging face - BERT</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gpt"><a class="header" href="#gpt">GPT</a></h1>
<p>Generative Pre-trained Transformer (GPT)</p>
<h2 id="gpt-2"><a class="header" href="#gpt-2">GPT-2</a></h2>
<p>GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset  of <strong>8 million web pages</strong>. 
GPT-2 is trained with a simple objective: </p>
<ul>
<li>predict the <strong>next word</strong>, given all of the previous words within some text. </li>
<li>The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. </li>
<li>GPT-2 is a direct scale-up of GPT, with more than <strong>10X the parameters</strong> and trained on more than 10X the amount of data.</li>
</ul>
<p>&quot; We (OpenAI) created a new dataset which emphasizes diversity of content, by scraping content from the Internet. In order to preserve document quality, we used only pages which have been <strong>curated/filtered by humans—specifically</strong>, we used outbound links from Reddit which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting (whether educational or funny), leading to higher data quality than other similar datasets, such as CommonCrawl.&quot;</p>
<p>GPT-2 algorithm was trained on the task of language modeling--- which tests a program's ability to predict the next word in a given sentence--by ingesting huge numbers of articles, blogs, and websites. By using just this data it achieved state-of-the-art scores on a number of unseen language tests, an achievement known as <strong>zero-shot learning</strong>. It can also perform other writing-related tasks, like translating text from one language to another, summarizing long articles, and answering trivia questions.</p>
<p><a href="https://github.com/openai/gpt-2">GPT-2 code</a>
<a href="https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb">Notebook</a></p>
<h3 id="zero-shot-learning"><a class="header" href="#zero-shot-learning">Zero-shot learning</a></h3>
<p>Zero-shot learning (ZSL) is a problem setup in machine learning, where <strong>at test time</strong>, a learner observes samples from classes that <strong>were not observed during training</strong>, and needs to predict the class they belong to.</p>
<h2 id="gpt-3"><a class="header" href="#gpt-3"><a href="https://github.com/openai/gpt-3">GPT-3</a></a></h2>
<p>Generative Pre-trained Transformer 3 is an <strong>autoregressive language model</strong> that uses deep learning to produce human-like text. </p>
<p>It is the third-generation language prediction model in the <em>GPT-n</em> series created by OpenAI.</p>
<p>GPT-3 is a very <strong>large language model</strong> (the largest till date) with about <a href="https://github.com/openai/gpt-3/blob/master/175b_samples.jsonl"><strong>175B parameters</strong></a>. It is trained on about <a href="https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_character_count.csv"><strong>45TB of text data</strong></a> from different datasets.</p>
<ul>
<li><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a></li>
<li><a href="https://paperswithcode.com/method/gpt-3">GPT-3 - paper with code</a></li>
<li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></li>
<li><a href="https://openai.com/blog/better-language-models/">Better Language Models and Their Implication</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="t5"><a class="header" href="#t5">T5</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="github-copilot"><a class="header" href="#github-copilot"><a href="https://copilot.github.com/">GitHub Copilot</a></a></h1>
<p>When we type this:</p>
<pre><code class="language-js">function calculateDaysBetweenDates(date1, date2) {

</code></pre>
<p>Copilot will complete this function:</p>
<pre><code class="language-js"> 
function calculateDaysBetweenDates(date1, date2) {

  // following lines are written by copilot
  var oneDay = 24 * 60 * 60 * 1000;
  var date1InMillis = date1.getTime();
  var date2InMillis = date2.getTime();
  var days = Math.round(Math.abs(date2InMillis - date1InMillis) / oneDay);
  return days;
}


</code></pre>
<p>\( \int x dx = \frac{x^2}{2} + C \)</p>
<p>\( \int y dy = \frac{y^3}{5} \)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references-1"><a class="header" href="#references-1">References</a></h1>
<h2 id="transformers"><a class="header" href="#transformers">Transformers</a></h2>
<ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></li>
</ul>
<iframe width="720" height="480" src="https://www.youtube.com/embed/LE3NfEULV6k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<hr />
<iframe width="720" height="480" src="https://www.youtube.com/embed/SZorAJ4I-sA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<ul>
<li><a href="https://www.youtube.com/watch?v=4Bdc55j80l8">Illustrated Guide to Transformers Neural Network: A step by step explanation</a></li>
<li><a href="https://www.youtube.com/watch?v=OyFJWRnt_AY">CS480/680 Lecture 19: Attention and Transformer Networks</a></li>
<li><a href="https://www.youtube.com/watch?v=S27pHKBEp30">LSTM is dead. Long Live Transformers!</a></li>
</ul>
<h2 id="open-ai"><a class="header" href="#open-ai">Open AI</a></h2>
<ul>
<li><a href="https://beta.openai.com/docs/introduction">OpenAI Documentation</a></li>
</ul>
<h3 id="codex"><a class="header" href="#codex">Codex</a></h3>
<ul>
<li>natural language to code</li>
<li><a href="https://www.youtube.com/watch?v=Zm9B-DvwOgw">Creating a Space Game with OpenAI Codex</a></li>
<li><a href="https://openai.com/blog/openai-codex/">OpenAI Codex</a></li>
</ul>
<h2 id="github-copilot-1"><a class="header" href="#github-copilot-1">Github Copilot</a></h2>
<ul>
<li>Trained on billions of lines of public code</li>
<li><a href="https://copilot.github.com/">Your AI pair programmer</a></li>
<li><a href="https://github.com/github/copilot-docs/blob/main/docs/visualstudiocode/gettingstarted.md#getting-started-with-github-copilot-in-visual-studio-code">VS Code extension</a></li>
</ul>
<h2 id="matplotlib"><a class="header" href="#matplotlib">Matplotlib</a></h2>
<p><a href="https://scriptverse.academy/tutorials/python-matplotlib-plot-function.html">Matplotlib: Plot a Function y=f(x) </a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="9-salesforce-einstein"><a class="header" href="#9-salesforce-einstein">9. Salesforce Einstein</a></h1>
<p>With Salesforce Einstein, we can: 
• Build custom predictions and recommendations with clicks
• Embed predictive insights into any record or in any app
• Operationalize AI by adding it to every workflow or business process</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="machine-learning"><a class="header" href="#machine-learning">Machine Learning</a></h1>
<h2 id="einstein-discovery"><a class="header" href="#einstein-discovery">Einstein Discovery</a></h2>
<p>Einstein Discovery automatically provides explanations and makes recommendations based on all your data sources so they can get smart insights, <strong>without the need of a data scientist</strong>.</p>
<h2 id="einstein-prediction-builder"><a class="header" href="#einstein-prediction-builder">Einstein Prediction Builder</a></h2>
<p>Einstein Prediction Builder helps you to predict the business outcomes, such as :</p>
<ul>
<li>churn or lifetime value. 
Create custom AI models on <strong>any Salesforce Object field</strong> or Object with <strong>clicks</strong>, not code.</li>
<li><a href="https://www.salesforce.com/content/dam/web/en_us/www/documents/e-books/analytics/sfdc-predictions.pdf">The big book of customer predictions - Get closer to your customers with Salesforce Einstein</a></li>
</ul>
<h2 id="einstein-next-best-action"><a class="header" href="#einstein-next-best-action">Einstein Next Best Action</a></h2>
<p>Einstein Next Best Action delivers proven recommendations to employees and customers, right in the apps where they work. </p>
<ul>
<li>Define recommendations, create <strong>action strategies</strong>, build predictive models, display recommendations, and activate automation.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="natural-language-processing"><a class="header" href="#natural-language-processing">Natural Language Processing</a></h1>
<h2 id="einstein-language"><a class="header" href="#einstein-language">Einstein Language</a></h2>
<p>Einstein Language helps you to understand:</p>
<ul>
<li>how <strong>customers feel</strong>, </li>
<li>automatically route inquiries</li>
<li>streamline your workflows. </li>
</ul>
<p>Build natural language processing (NLP) into your apps to classify the <strong>underlying intent</strong> and <strong>sentiment</strong> in a body of text, <strong>no matter what the language</strong>.</p>
<h2 id="einstein-bots"><a class="header" href="#einstein-bots">Einstein Bots</a></h2>
<p>Einstein Bots helps you to easily build, train, and deploy <strong>custom bots on digital channels</strong> that are connected to your CRM data. </p>
<ul>
<li>Enhance business processes, empower your employees, and delight your customers.</li>
<li><a href="https://www.salesforce.com/content/dam/web/en_us/www/documents/datasheets/einstein-for-service-data-sheet.pdf">Einstein for Service: AI-POWERED CUSTOMER SERVICE</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="computer-vision"><a class="header" href="#computer-vision">Computer Vision</a></h1>
<h2 id="einstein-vision"><a class="header" href="#einstein-vision">Einstein Vision</a></h2>
<p>Einstein Vision helps you to see the entire conversation about your brand on social media and beyond. </p>
<ul>
<li>Use intelligent image recognition in your apps by training deep learning models to <strong>recognize your brand, products</strong>, and more.</li>
<li><a href="https://www.salesforce.com/content/dam/web/en_us/www/assets/pdf/datasheets/einstein-vision-and-language-getting-started.pdf">Get Started Using Deep Learning for Business Users &amp; Techies</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="10-google-cloud-platform"><a class="header" href="#10-google-cloud-platform">10. Google Cloud Platform</a></h1>
<h2 id="dataflow"><a class="header" href="#dataflow">Dataflow</a></h2>
<p><img src="img/gcp/gcp-df-1.png" alt="GCP Dataflow" /></p>
<iframe width="720" height="480" src="https://www.youtube.com/embed/udKgN1_eThs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="11-processing-units"><a class="header" href="#11-processing-units">11. Processing Units</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cpu"><a class="header" href="#cpu">CPU</a></h1>
<p>CPU is constructed from millions of transistors.
It can have <strong>multiple processing cores</strong> and is commonly referred to as the brain of the computer. 
It is essential to all modern computing systems as it executes the commands and processes needed for your computer and operating system. 
The CPU is also important in determining how fast programs can run, from surfing the web to building spreadsheets.</p>
<p>The CPU is suited to a wide variety of workloads, especially those for which latency or per-core performance are important. A powerful execution engine, the CPU focuses its **smaller number of cores on individual tasks and on getting things done quickly. This makes it uniquely well equipped for jobs ranging from serial computing to running databases.</p>
<h2 id="intels-sandy-bridge-architecture--32-nm-micro-architecture"><a class="header" href="#intels-sandy-bridge-architecture--32-nm-micro-architecture">Intel's Sandy Bridge Architecture ( 32 nm micro architecture)</a></h2>
<p><img src="img/cpu/cpu-intel-sandy-bridge-1.png" alt="Intel Sandy Bridge CPU" /></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Sandy_Bridge#:%7E:text=Sandy%20Bridge%20is%20the%20codename,to%20Nehalem%20and%20Westmere%20microarchitecture.">Refer</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gpu"><a class="header" href="#gpu">GPU</a></h1>
<p>The GPU is a processor that is made up of many smaller and more <strong>specialized cores</strong>. By working together, these cores deliver <strong>massive performance</strong> when a processing task can be <strong>divided up</strong> and processed across these cores.</p>
<p>GPUs began as specialized <a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit">ASICs - Application Specific Integrated Circuit</a> developed to <strong>accelerate specific 3D rendering tasks</strong>. </p>
<p>Over time, these fixed-function engines became more programmable and more flexible. While graphics and the increasingly lifelike visuals of today’s top games remain their principal function, GPUs have evolved to <strong>become more general-purpose parallel processors as well</strong>, handling a growing range of applications.</p>
<p>Initially GPUs were solving computer <strong>graphics</strong> related problems in Gaming
The General Purpose GPU (GPGPU) plays a vital role in the deep learning and parallel computing.</p>
<p><img src="img/gpu/NVIDIA-GPU-arch.png" alt="NIVIDA GPU" /></p>
<h3 id="what-is-cuda"><a class="header" href="#what-is-cuda">What is CUDA?</a></h3>
<p>Compute Unified Device Architecture (<a href="https://developer.nvidia.com/blog/cuda-refresher-getting-started-with-cuda/">CUDA</a>) is is a parallel computing platform developed by NVIDIA. It enables software programs to perform calculations using both the CPU and GPU.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tpu---tensor-processing-units"><a class="header" href="#tpu---tensor-processing-units">TPU - Tensor Processing Units</a></h1>
<p><a href="https://cloud.google.com/tpu/docs/tpus">TPUs</a> are Google’s custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads.</p>
<p>Designed from the ground up with the benefit of Google’s deep experience and leadership in machine learning.</p>
<p>Enable us to run our machine learning workloads on <strong>Google’s TPU accelerator hardware</strong> using <strong>TensorFlow</strong></p>
<p>Designed for maximum performance and flexibility to help researchers, developers, and businesses to build TensorFlow compute clusters that can leverage <strong>CPUs, GPUs, and TPUs</strong>.</p>
<p>High-level <strong>TensorFlow APIs</strong> help us to get models running on the Cloud TPU hardware.</p>
<h2 id="advantages-for-using-tpus"><a class="header" href="#advantages-for-using-tpus">Advantages for using TPUs</a></h2>
<ul>
<li>
<p>Cloud TPU resources accelerate the performance of <strong>linear algebra computation</strong>, which is used heavily in machine learning applications.</p>
</li>
<li>
<p>TPUs <strong>minimize</strong> the <strong>time-to-accuracy</strong> when you <strong>train large, complex neural network models</strong>. Weeks to hours (150 times faster)</p>
<ul>
<li>Models that previously took <strong>weeks to train</strong> on other hardware platforms can converge in <strong>hours on TPUs</strong>.</li>
</ul>
</li>
</ul>
<h2 id="tpu-v3"><a class="header" href="#tpu-v3">TPU v3</a></h2>
<p>A <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">TPU v3</a> board contains four TPU chips and 32 GiB of HBM. Each TPU chip contains two cores. Each core has a MXU, a vector unit, and a scalar unit.</p>
<p><img src="https://cloud.google.com/tpu/docs/images/tpu-v3-layout.png" alt="TPU architecture" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="11-ml-pipelines"><a class="header" href="#11-ml-pipelines">11. ML Pipelines</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ml-ops-or-mlops"><a class="header" href="#ml-ops-or-mlops">ML ops or MLOps</a></h1>
<p>Set of practices that aims to deploy and maintain machine learning models in <strong>production reliably and efficiently</strong>.</p>
<p>The word <strong>MLOps</strong> is a compound of <em>machine learning</em> and the continuous development practice of <em>DevOps</em> in the software field. </p>
<ul>
<li>Machine learning models are tested and developed in isolated experimental systems.</li>
<li>When an algorithm is ready to be launched, MLOps is practiced between Data Scientists, DevOps, and Machine Learning engineers to transition the algorithm to production systems.</li>
<li>MLOps seeks to increase automation and improve the quality of production models, while also focusing on business and regulatory requirements.</li>
</ul>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/ML_Ops_Venn_Diagram.svg" alt="MLOps Wiki" />
<img src="https://ml-ops.org/img/mlops-loop-en.jpg" alt="MlOps" /></p>
<iframe width="800" height="420" src="https://www.youtube.com/embed/Ta14KpeZJok" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="papers-1"><a class="header" href="#papers-1">Papers</a></h2>
<ul>
<li><a href="https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">Hidden Technical Debt in Machine Learning Systems</a></li>
</ul>
<h2 id="salesforce-transmogrifai"><a class="header" href="#salesforce-transmogrifai">Salesforce TransmogrifAI</a></h2>
<ul>
<li>AutoML library for building modular, reusable, strongly typed machine learning workflows on Apache Spark with minimal hand-tuning</li>
<li>Transmogrification as the process of transforming, often in a surprising or magical manner, which is what TransmogrifAI does for Salesforce 
<ul>
<li>enabling data science teams to transform customer data into meaningful, actionable predictions</li>
<li>thousands of <strong>customer-specific machine learning models</strong> have been deployed across the platform, powering more than 3 billion predictions every day.</li>
</ul>
</li>
</ul>
<p>TransmogrifAI is a library built on Scala and SparkML that does precisely this. </p>
<p>With just a few lines of code, a data scientist can automate data cleansing, feature engineering, and model selection to arrive at a performant model from which she can explore and iterate further.</p>
<p><img src="https://miro.medium.com/max/5280/1*nRxOm1irE_aNB-UweA7IXA.png" alt="TransmogrifAI" /></p>
<p>Type Safety: The TransmogrifAI Feature type hierarchy</p>
<p><img src="https://miro.medium.com/max/1540/1*vkrbLrOyIinhonrPCGHivA.png" alt="Type Safety" /></p>
<p>Transmogrification
These transformations are not just about getting the data into a format which algorithms can use, TransmogrifAI also optimizes the transformations to make it easier for machine learning algorithms to learn from the data.</p>
<ul>
<li>For example, it might transform a numeric feature like age into the most appropriate age buckets for a particular problem — age buckets for the fashion industry might differ from wealth management age buckets.</li>
</ul>
<p>TransgmogrifAI has algorithms that perform automatic feature validation to remove features with little to <em>no predictive power</em>.
- Example: <em>Closed Deal Amount</em></p>
<p>The TransmogrifAI <em>Model Selector</em> runs a tournament of several different machine learning algorithms on the data and uses the average validation error to automatically choose the best one</p>
<p>TransmogrifAI comes with some techniques for automatically tuning these hyperparameters and a framework to extend to more advanced tuning techniques.</p>
<pre><code class="language-scala">
// Read the Deal data
val dealData = DataReaders.Simple.csvCase[Deal](path = pathToData).readDataset().toDF()

// Extract response and predictor Features
val (isClosed, predictors) = FeatureBuilder.fromDataFrame[RealNN](dealData, response = &quot;isClosed&quot;)

// Automated feature engineering
val featureVector = predictors.transmogrify()

// Automated feature validation
val cleanFeatures = isClosed.sanityCheck(featureVector, removeBadFeatures = true)

// Automated model selection
val (pred, raw, prob) = BinaryClassificationModelSelector().setInput(isClosed, cleanFeatures).getOutput()

// Setting up the workflow and training the model
val model = new OpWorkflow().setInputDataset(dealData).setResultFeatures(pred).train()

</code></pre>
<p>TransmogrifAI is built  on top of Apache Spark</p>
<ul>
<li>
<p>Able to handle large variation in the size of the data</p>
<ul>
<li>Some use cases involve tens of millions of records that need to be aggregated or joined, others depend on a few thousands of records.</li>
</ul>
</li>
<li>
<p>Spark has primitives for dealing with distributed joins and aggregates on big data </p>
</li>
<li>
<p>Able to serve our machine learning models in both a batch and streaming (Spark Streaming) setting</p>
</li>
<li>
<p>Transmogrification, Feature Validation, and Model Selection above, are all powered by Estimators)</p>
<ul>
<li>A Feature is essentially a type-safe pointer to a column in a DataFrame and contains all the information about that column — its name, the type of data it contains, as well as lineage information about how it was derived.</li>
</ul>
</li>
<li>
<p>TransmogrifAI provides the ability to easily define features that are the result of complex time-series aggregates and joins</p>
</li>
<li>
<p>Features are strongly typed. This allows TransmogrifAI to do type checks on the entire machine learning workflow, and ensure that <em>errors are caught as early on as possible</em> instead of hours into a running pipeline</p>
</li>
<li>
<p>Developers can easily specify custom transformers and estimators to be used in the pipeline</p>
</li>
</ul>
<pre><code class="language-scala">val lowerCaseText = textFeature.map[Text](_.value.map(_.toLowerCase).toText)
</code></pre>
<h3 id="scale-and-performance"><a class="header" href="#scale-and-performance">Scale and performance</a></h3>
<ul>
<li>With automated feature engineering, data scientists can easily <em>blow up the feature space</em>, and end up with wide DataFrames that are hard for Spark to deal with. </li>
<li>TransmogrifAI workflows address this by inferring the entire DAG of transformations that are needed to materialize features, and optimize the execution of this DAG by collapsing all transformations that occur at the same level of the DAG into a single operation.</li>
</ul>
<h3 id="summary"><a class="header" href="#summary">Summary</a></h3>
<p>TransmogrifAI enables our data scientists to deploy thousands of models in production with minimal hand tuning and reducing the average turn-around time for training a performant model from weeks to just a couple of hours.</p>
<p><a href="https://github.com/salesforce/TransmogrifAI">TransmogrifAI</a></p>
<ul>
<li><a href="https://engineering.salesforce.com/open-sourcing-transmogrifai-4e5d0e098da2">Open Sourcing TransmogrifAI</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensorflow-serving"><a class="header" href="#tensorflow-serving">TensorFlow Serving</a></h1>
<p>TensorFlow Serving is a flexible, <strong>high-performance serving system</strong> for machine learning models, designed for <strong>production environments</strong>. </p>
<p>TensorFlow Serving makes it easy to <strong>deploy new algorithms and experiments</strong>, while keeping the same server architecture and APIs. </p>
<p><a href="https://www.tensorflow.org/tfx/serving/architecture">TensorFlow Serving</a> provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve <strong>other types of models and data</strong>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensorflow-extended-tfx"><a class="header" href="#tensorflow-extended-tfx">TensorFlow Extended (TFX)</a></h1>
<p>TFX is a Google-production-scale machine learning (ML) platform based on TensorFlow. </p>
<p>It provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor your machine learning system.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="apache-airflow"><a class="header" href="#apache-airflow">Apache Airflow</a></h1>
<p>Apache Airflow is a platform to programmatically author, schedule and monitor workflows. 
TFX (TensorFlow Extended) uses <strong>Airflow</strong> to author workflows as directed acyclic graphs (<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAGs</a>) of tasks. </p>
<p>The Airflow <strong>scheduler executes tasks</strong> on an array of workers while following the specified dependencies. </p>
<p>Rich command line utilities (CLI) make performing complex surgeries on DAGs a snap. 
The rich user interface (UI) makes it easy to:</p>
<ul>
<li><strong>visualize pipelines</strong> running in production, </li>
<li><strong>monitor progress</strong>, </li>
<li><strong>troubleshoot issues</strong> when needed. </li>
</ul>
<p>When workflows are <strong>defined as code</strong>, they become more maintainable, versionable, testable, and collaborative.</p>
<h3 id="dag"><a class="header" href="#dag">DAG</a></h3>
<p>DAG (directed acyclic graph) is a directed graph with no directed cycles. That is, it consists of vertices and edges, with each edge directed from one vertex to another, such that following those directions will <strong>never form a closed loop</strong>. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="apache-beam"><a class="header" href="#apache-beam">Apache Beam</a></h1>
<p>Apache Bean help us to implement <strong>batch and streaming data processing jobs</strong> that run on any execution engine.</p>
<p>Several TFX (TensorFlow Extended) components rely on Beam for <strong>distributed data processing</strong>. 
In addition, TFX can use Apache Beam to orchestrate and execute the pipeline DAG. </p>
<p>Beam orchestrator uses a different <strong>BeamRunner</strong> than the one which is used for component data processing. 
With the default <strong>DirectRunner</strong> setup the Beam orchestrator can be used for local debugging without incurring the extra <strong>Airflow or Kubeflow dependencies</strong>, which simplifies system configuration.</p>
<p><img src="img/apachebeam/beam-1.webp" alt="Apache Beam" /></p>
<iframe width="720" height="480" src="https://www.youtube.com/embed/udKgN1_eThs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kubeflow"><a class="header" href="#kubeflow">Kubeflow</a></h1>
<p>The Kubeflow project is dedicated to making <strong>deployments of machine learning (ML) workflows</strong> on <strong>Kubernetes</strong> simple, portable and scalable.</p>
<p><img src="img/kubeflow/kubeflow-arch-1.svg" alt="Kubeflow Arch" /></p>
<p>Kubeflow is an open source ML platform dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. </p>
<p>Kubeflow Pipelines is part of the <strong>Kubeflow platform</strong> that enables composition and execution of reproducible workflows on Kubeflow, integrated with experimentation and notebook based experiences. </p>
<p>Kubeflow Pipelines services on Kubernetes include the hosted Metadata store, container based orchestration engine, notebook server, and UI to help users develop, run, and manage complex ML pipelines at scale. </p>
<p>The Kubeflow Pipelines SDK allows for creation and sharing of components and composition and of pipelines programmatically.</p>
<h2 id="kubernetes"><a class="header" href="#kubernetes">Kubernetes</a></h2>
<p>Kubernetes is an open-source <strong>container-orchestration system</strong> for automating computer application </p>
<ul>
<li>deployment</li>
<li>scaling</li>
<li>management</li>
</ul>
<p>It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation.</p>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<ul>
<li><a href="https://www.kubeflow.org/docs/started/introduction/">Kubeflow home</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="automl"><a class="header" href="#automl">AutoML</a></h1>
<p>AutoML makes the power of machine learning available to you even if you have limited knowledge of machine learning.</p>
<p>You can use AutoML to build on Google's machine learning capabilities to <strong>create your own custom machine learning models that are tailored to your business needs</strong>, and then integrate those models into your applications and web sites.</p>
<iframe width="800" height="420" src="https://www.youtube.com/embed/93vsqjfGPCw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="800" height="420" src="https://www.youtube.com/embed/PKTvo9X9Sjg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="12-speedup"><a class="header" href="#12-speedup">12. Speedup</a></h1>
<p>In this chapter we will see the ways to speed up Machine Learning</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="12-jax"><a class="header" href="#12-jax">12. JAX</a></h1>
<p><img src="https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png" alt="JAX" /></p>
<p><a href="https://github.com/google/jax">JAX</a> is a new library from <a href="https://research.google/">Google Research</a>. 
JAX can automatically differentiate native Python and Numpy functions.</p>
<ul>
<li>Loops</li>
<li>Branches</li>
<li>Recursion </li>
<li><a href="closure.html">Closures</a></li>
<li>Can take Derivative of Derivatives
<ul>
<li>Supports <strong>reverse mode differentiation</strong>, also known as [Back Propagation] using Grad function</li>
<li>Supports <strong>forward mode differentiation</strong></li>
</ul>
</li>
</ul>
<h2 id="xla"><a class="header" href="#xla">XLA</a></h2>
<p><a href="https://www.tensorflow.org/xla">XLA</a> is <strong>Accelerated Linear Algebra</strong>. </p>
<ul>
<li>
<p>It is a <strong>domain-specific compiler</strong> for <strong>linear algebra</strong> that can <strong>accelerate TensorFlow models</strong> with potentially <strong>no source code changes</strong>.</p>
</li>
<li>
<p>Performs optimizations like:</p>
<ul>
<li><strong>Fusing</strong> operations together (something like consolidation) so the intermediate results do not have to written out the memory. Instead it get <strong>streamed</strong> into next operation.</li>
<li>This enable faster and more efficient processing</li>
</ul>
<p>This is some what <strong>crudely</strong> equal to nodejs stream:
- Refer: TableauCRM CLI using this stream concept, where it loads data from a Oracle SQL Query results directly into Tableau CRM dataset
- <a href="https://github.com/mohan-chinnappan-n/cli-dx/blob/master/db/ora2ea.md">refer: sfdx mohanc:ea:dataset:loadFromOra</a></p>
<pre><code class="language-py"> def model (x, y, z):
     return tf.reduce_sum( x + y * z)
</code></pre>
</li>
</ul>
<p>JAX uses XLA to compile and run our Numpy program on <a href="https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html">GPUs</a> and <a href="https://cloud.google.com/tpu/docs/tpus">TPUs</a></p>
<p>JAX uses JIT (just-in-time) compile of custom functions into XLA optimized kernels using decorator <code>@jit </code></p>
<pre><code class="language-py">
@jit # jit decorator
def update(params, x, y):
    grads = gard(loss)(params, x, y)
    return [ (w - step_size * dw, b - step_size * db) for (w, b), (dw, db) in zip (params, grads)]

</code></pre>
<h2 id="pmap"><a class="header" href="#pmap">pmap</a></h2>
<ul>
<li>JAX applies pmap (<a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap">Parallel Map</a>) replicating computations across multiple cores
<img src="img/jax/jax-pmap.png" alt="pmap" /></li>
</ul>
<h2 id="autograd"><a class="header" href="#autograd">Autograd</a></h2>
<p>Autograd (https://github.com/hips/autograd) can automatically differentiate native <strong>Python and Numpy</strong> code.</p>
<h3 id="functions-available-for-the-transformations"><a class="header" href="#functions-available-for-the-transformations">Functions available for the transformations</a></h3>
<ul>
<li>grad</li>
<li>jit</li>
<li>pmap</li>
<li>vmap - automatic vectorization
<ul>
<li>allowing us to turn a function which can handle only <strong>one data point</strong> into a function which can handle <strong>batch of these data points</strong> of any size with just one wrapper function <em>vmap</em></li>
</ul>
</li>
</ul>
<h3 id="sample---mnist"><a class="header" href="#sample---mnist"><a href="https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/neural_network_with_tfds_data.ipynb">Sample - MNIST </a></a></h3>
<ul>
<li><a href="http://yann.lecun.com/exdb/mnist/">MIST Database</a></li>
</ul>
<pre><code class="language-py">
import jax.numpy as jnp
from jax import grad, vmap, jit


def predict(params, inputs):
  for W, b in params:
    outputs = jnp.dot(inputs, W) + b
    inputs = jnp.tanh(outputs)
  return outputs

def loss (params, batch):
  inputs, targets = batch
  preds = predict(params, inputs)
  return jnp.sum( (preds - targets) **2 ) # SME


gradient_fun = jit(grad(loss))
preexample_grads = vmap(grad(loss), in_axes=(None, 0))

</code></pre>
<h3 id="key-ideas"><a class="header" href="#key-ideas">Key Ideas</a></h3>
<ul>
<li>Python code is traced into an Intermediate Representation (IR)
<ul>
<li>IR can be transformed (automatic differentiation) </li>
<li>IR enables domain-specific compilation (XLA - Accelerated Linear Algebra)</li>
</ul>
</li>
<li>Has very powerful transforms
<ul>
<li>grad</li>
<li>jit</li>
<li>vmap</li>
<li>pmap</li>
</ul>
</li>
<li>Python's dynamism makes this possible
<ul>
<li>JAX makes use of this dynamism and evaluates a function's behavior by calling it on a <strong>tracer</strong> value</li>
</ul>
</li>
</ul>
<pre><code class="language-py">def sum(x):
    return x + 2

class ShapedArray(object):
    def __add__ (self, other):
        self.record_computation(&quot;add&quot;, self, other)
        return ShapedArray(self.shape, self.dtype) # dtype is like float32

sum( ShapedArray( (2,2), float32 ))


</code></pre>
<p><img src="img/jax/jax-ir-1.png" alt="jax-IR" /></p>
<p>With this IR, JAX knows how to do the transforms like:</p>
<ul>
<li>grad</li>
<li>jit</li>
<li>vmap</li>
<li>pmap</li>
</ul>
<p><img src="img/jax/jax-ir-2.png" alt="jax-IR" /></p>
<p><img src="img/jax/jax-ir-3.png" alt="jax-IR" /></p>
<pre><code class="language-py">TF_CPP_MIN_LOG_LEVEL=0 

import jax
import jax.numpy as jnp

global_list = []

def log2(x):
  global_list.append(x)
  ln_x = jnp.log(x)
  ln_2 = jnp.log(2.0)
  return ln_x / ln_2

print( jax.make_jaxpr(log2)(3.0) )

</code></pre>
<ul>
<li><a href="https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#how-jax-transforms-work">Document</a></li>
<li>Output </li>
</ul>
<pre><code>{ lambda ; a:f32[]. let
    b:f32[] = log a
    c:f32[] = log 2.0
    d:f32[] = div b c
  in (d,) 
}

</code></pre>
<h3 id="jake-on-jax"><a class="header" href="#jake-on-jax">Jake on JAX</a></h3>
<iframe width="720" height="480" src="https://www.youtube.com/embed/WdTeDXsOSj4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/0mVmRHMaOJ4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/fuAyUQcVzTY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="closures-and-decorators"><a class="header" href="#closures-and-decorators">Closures and Decorators</a></h1>
<h2 id="python-closures"><a class="header" href="#python-closures">Python Closures</a></h2>
<p>Let us explain <strong>closure</strong> by an example:</p>
<ul>
<li><a href="https://www.programiz.com/python-programming/closure">Refer</a></li>
</ul>
<pre><code class="language-py">
# This is the outer enclosing function
def print_msg(msg):

    def printer():
        # This is the nested function
        print(msg)

    return printer  # returns the nested function


# Now let's try calling this function.
another = print_msg(&quot;Hello&quot;)
another()
# Output: Hello
</code></pre>
<p>This technique by which <strong>some data</strong> in our case &quot;Hello&quot; gets attached to the code - <em>another()</em> is called <strong>closure</strong> in Python.</p>
<p>Three characteristics of a Python closure are: </p>
<ol>
<li>it is a nested function, in our example: <em>printer()</em> </li>
<li>it has access to a free variable in outer scope, in our example: <em>msg</em>. </li>
<li>it is returned from the <strong>enclosing</strong> function, in our example: <em>print_msg()</em></li>
</ol>
<pre><code>

``
# Python Decorators make an extensive use of closures 

</code></pre>
<h2 id="python-decorators"><a class="header" href="#python-decorators">Python Decorators</a></h2>
<ul>
<li><a href="https://www.programiz.com/python-programming/decorator">Refer</a></li>
</ul>
<p>A decorator takes in a function, adds some functionality and returns it. </p>
<pre><code class="language-py">#  a decorator takes in a function, adds some functionality and returns it.

# takes in function to be decorated
def make_pretty(func):
    def inner():
        print(&quot;I got decorated&quot;) # getting decorated
        func() # back to the given function
    return inner


def ordinary():
    print(&quot;I am ordinary&quot;)

</code></pre>
<pre><code class="language-py"># will print:  I am ordinary

ordinary()

</code></pre>
<pre><code class="language-py">decorated = make_pretty(ordinary)
decorated() 
&quot;&quot;&quot; will print: 
    I got decorated
    I am ordinary
&quot;&quot;&quot;

# decorator function (make_pretty) has added
##  some new functionality to the original function (ordinary)

</code></pre>
<pre><code class="language-py"># annoation way
@make_pretty # syntactic sugar
def ordinary():
    print(&quot;I am ordinary&quot;)

</code></pre>
<pre><code class="language-py">iam_special = ordinary()
&quot;&quot;&quot; will print: 
    I got decorated
    I am ordinary
&quot;&quot;&quot;

</code></pre>
<h3 id="decorating-functions-with-parameters"><a class="header" href="#decorating-functions-with-parameters">Decorating functions with parameters</a></h3>
<pre><code class="language-py"># Decorating functions with parameters

def smart_divide(func):
    def inner(a, b):
        print(&quot;I am going to divide&quot;, a, &quot;and&quot;, b)
        if b == 0:
            print(&quot;Whoops! cannot divide by zero&quot;)
            return

        return func(a, b)
    return inner


@smart_divide
def divide(a, b):
    print(a/b)
</code></pre>
<pre><code class="language-py">
divide(10,2)

&quot;&quot;&quot; will print: 
    I am going to divide 10 and 2
    5.0
&quot;&quot;&quot;
</code></pre>
<pre><code class="language-py">
divide(10,0)

&quot;&quot;&quot; will print: 
    I am going to divide 10 and 0
    Whoops! cannot divide by zero
&quot;&quot;&quot;

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references-3"><a class="header" href="#references-3">References</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/0mVmRHMaOJ4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<ul>
<li><a href="https://realpython.com/primer-on-python-decorators/">Primer on Python Decorators</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="13-openai"><a class="header" href="#13-openai">13. OpenAI</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openai-api"><a class="header" href="#openai-api">OpenAI API</a></h1>
<h3 id="completion"><a class="header" href="#completion">Completion</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/engines/davinci/completions \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY \
  -d '{
  &quot;prompt&quot;: &quot;Once upon a time&quot;,
  &quot;max_tokens&quot;: 5
}'

</code></pre>
<ul>
<li>Result</li>
</ul>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;cmpl-4DmYlIcNgBh26avH8t5mMMWwgILGE&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;created&quot;: 1639190275,
  &quot;model&quot;: &quot;davinci:2020-05-03&quot;,
  &quot;choices&quot;: [
    {
      &quot;text&quot;: &quot;, there was a software&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;finish_reason&quot;: &quot;length&quot;
    }
  ]
}

</code></pre>
<h3 id="search"><a class="header" href="#search">Search</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/engines/davinci/search \
  -H &quot;Content-Type: application/json&quot; \
  -H 'Authorization: Bearer YOUR_API_KEY' \
  -d '{
  &quot;documents&quot;: [&quot;White House&quot;, &quot;hospital&quot;, &quot;school&quot;],
  &quot;query&quot;: &quot;the president&quot;
}'
</code></pre>
<pre><code class="language-json">{
  &quot;object&quot;: &quot;list&quot;,
  &quot;data&quot;: [
    {
      &quot;object&quot;: &quot;search_result&quot;,
      &quot;document&quot;: 0,
      &quot;score&quot;: 215.56
    },
    {
      &quot;object&quot;: &quot;search_result&quot;,
      &quot;document&quot;: 1,
      &quot;score&quot;: 55.614
    },
    {
      &quot;object&quot;: &quot;search_result&quot;,
      &quot;document&quot;: 2,
      &quot;score&quot;: 40.932
    }
  ],
  &quot;model&quot;: &quot;davinci:2020-05-03&quot;
}

</code></pre>
<h3 id="create-classification"><a class="header" href="#create-classification">Create Classification</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/classifications \
  -X POST \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -H 'Content-Type: application/json' \
  -d '{
    &quot;examples&quot;: [
      [&quot;A happy moment&quot;, &quot;Positive&quot;],
      [&quot;I am sad.&quot;, &quot;Negative&quot;],
      [&quot;I am feeling awesome&quot;, &quot;Positive&quot;]],
    &quot;query&quot;: &quot;It is a raining day :(&quot;,
    &quot;search_model&quot;: &quot;ada&quot;,
    &quot;model&quot;: &quot;curie&quot;,
    &quot;labels&quot;:[&quot;Positive&quot;, &quot;Negative&quot;, &quot;Neutral&quot;]
  }'

</code></pre>
<pre><code class="language-json">
{
  &quot;completion&quot;: &quot;cmpl-4DmdStcV7tC6o5VJnoihr4TDFHse4&quot;,
  &quot;label&quot;: &quot;Negative&quot;,
  &quot;model&quot;: &quot;curie:2020-05-03&quot;,
  &quot;object&quot;: &quot;classification&quot;,
  &quot;search_model&quot;: &quot;ada&quot;,
  &quot;selected_examples&quot;: [
    {
      &quot;document&quot;: 1,
      &quot;label&quot;: &quot;Negative&quot;,
      &quot;text&quot;: &quot;I am sad.&quot;
    },
    {
      &quot;document&quot;: 0,
      &quot;label&quot;: &quot;Positive&quot;,
      &quot;text&quot;: &quot;A happy moment&quot;
    },
    {
      &quot;document&quot;: 2,
      &quot;label&quot;: &quot;Positive&quot;,
      &quot;text&quot;: &quot;I am feeling awesome&quot;
    }
  ]
}


</code></pre>
<h3 id="answers"><a class="header" href="#answers">Answers</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/answers \
  -X POST \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -H 'Content-Type: application/json' \
  -d '{
    &quot;documents&quot;: [&quot;Puppy A is happy.&quot;, &quot;Puppy B is sad.&quot;],
    &quot;question&quot;: &quot;which puppy is happy?&quot;,
    &quot;search_model&quot;: &quot;ada&quot;,
    &quot;model&quot;: &quot;curie&quot;,
    &quot;examples_context&quot;: &quot;In 2017, U.S. life expectancy was 78.6 years.&quot;,
    &quot;examples&quot;: [[&quot;What is human life expectancy in the United States?&quot;,&quot;78 years.&quot;]],
    &quot;max_tokens&quot;: 5,
    &quot;stop&quot;: [&quot;\n&quot;, &quot;&lt;|endoftext|&gt;&quot;]
  }'

</code></pre>
<pre><code class="language-json">{
&quot;answers&quot;: [
  &quot;puppy A.&quot;
],
&quot;completion&quot;: &quot;cmpl-4DmgSrZJ7sQx6lWRbaMyskSN68qCE&quot;,
&quot;model&quot;: &quot;curie:2020-05-03&quot;,
&quot;object&quot;: &quot;answer&quot;,
&quot;search_model&quot;: &quot;ada&quot;,
&quot;selected_documents&quot;: [
  {
    &quot;document&quot;: 0,
    &quot;text&quot;: &quot;Puppy A is happy. &quot;
  },
  {
    &quot;document&quot;: 1,
    &quot;text&quot;: &quot;Puppy B is sad. &quot;
  }
]
}

</code></pre>
<h3 id="list-files"><a class="header" href="#list-files">List Files</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/files \
  -H 'Authorization: Bearer YOUR_API_KEY'


</code></pre>
<pre><code class="language-json">{
  &quot;object&quot;: &quot;list&quot;,
  &quot;data&quot;: []
}

</code></pre>
<h3 id="upload-files"><a class="header" href="#upload-files">Upload Files</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/files \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -F purpose=&quot;answers&quot; \
  -F file='@puppy.jsonl'

</code></pre>
<h3 id="delete-file"><a class="header" href="#delete-file">Delete File</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/files/file-XjGxS3KTG0uNmNOK362iJua3 \
  -X DELETE \
  -H 'Authorization: Bearer YOUR_API_KEY'

</code></pre>
<h3 id="retrieve-file-information"><a class="header" href="#retrieve-file-information">Retrieve File Information</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/files/file-XjGxS3KTG0uNmNOK362iJua3 \
  -H 'Authorization: Bearer YOUR_API_KEY'

</code></pre>
<h3 id="retrieve-file-content"><a class="header" href="#retrieve-file-content">Retrieve File Content</a></h3>
<pre><code class="language-bash">
curl https://api.openai.com/v1/files/file-XjGxS3KTG0uNmNOK362iJua3/content \
  -H 'Authorization: Bearer YOUR_API_KEY' &gt; file.jsonl


</code></pre>
<h3 id="fine-tunes"><a class="header" href="#fine-tunes">Fine Tunes</a></h3>
<ul>
<li>Manage fine-tuning jobs to tailor a model to your specific training data.</li>
</ul>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes \
  -X POST \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -d '{
  &quot;training_file&quot;: &quot;file-XGinujblHPwGLSztz8cPS8XY&quot;
}'

</code></pre>
<h3 id="list-fine-tunes"><a class="header" href="#list-fine-tunes">List fine-tunes</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes \
  -H 'Authorization: Bearer YOUR_API_KEY'

</code></pre>
<h3 id="list-fine-tune-information"><a class="header" href="#list-fine-tune-information">List fine-tune information</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes/ftjob-AF1WoRqd3aJAHsqc9NY7iL8F \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot;

</code></pre>
<h3 id="cancel-a-fine-tune"><a class="header" href="#cancel-a-fine-tune">Cancel a fine-tune</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes/ftjob-AF1WoRqd3aJAHsqc9NY7iL8F/cancel \
  -X POST \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot;

</code></pre>
<h3 id="list-fine-tune-events"><a class="header" href="#list-fine-tune-events">List fine-tune events</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes/ftjob-AF1WoRqd3aJAHsqc9NY7iL8F/events \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot;

</code></pre>
<h3 id="embeddings"><a class="header" href="#embeddings">Embeddings</a></h3>
<ul>
<li>Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.</li>
</ul>
<pre><code class="language-bash">curl https://api.openai.com/v1/engines/babbage-similarity/embeddings \
  -X POST \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d '{&quot;input&quot;: &quot;The food was delicious and the waiter...&quot;}' 

</code></pre>
<pre><code class="language-json">{
  &quot;object&quot;: &quot;list&quot;,
  &quot;data&quot;: [
    {
      &quot;object&quot;: &quot;embedding&quot;,
      &quot;embedding&quot;: [
        0.002866707742214203,
        0.01886799931526184,
        -0.03013569489121437,
        -0.004034548997879028,
        ...
      ]
     &quot;index&quot;: 0
    }
  ],
  &quot;model&quot;: &quot;babbage-similarity-model:2021-09-20&quot;
}

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chat"><a class="header" href="#chat">Chat</a></h1>
<p><img src="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/openAI/openAI-chat-1.webm.gif" alt="OpenAI chat" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summarize"><a class="header" href="#summarize">Summarize</a></h1>
<p><img src="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/openAI/openAI-summarize-1.webm.gif" alt="OpenAI chat" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tldr"><a class="header" href="#tldr">TLDR</a></h1>
<h1 id="translate"><a class="header" href="#translate">Translate</a></h1>
<p><img src="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/openAI/openAI-tldr-1.webm.gif" alt="OpenAI chat" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="translate-1"><a class="header" href="#translate-1">Translate</a></h1>
<p><img src="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/openAI/openAI-translate-en-fr-1.webm.gif" alt="OpenAI chat" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="codex-1"><a class="header" href="#codex-1">Codex</a></h1>
<iframe src='https://mohan-chinnappan-n2.github.io/2021/ai/openai/codex/1.html' 
width="860" height="500">
</iframe>
<h2 id="demo-javascript-code-writing-with-codex"><a class="header" href="#demo-javascript-code-writing-with-codex"><a href="https://mohan-chinnappan-n2.github.io/2021/ai/openai/codex/img/1.webm">Demo: Javascript code writing with Codex</a></a></h2>
<h2 id="videos-2"><a class="header" href="#videos-2">Videos</a></h2>
<iframe width="720" height="480" src="https://www.youtube.com/embed/SGUCcjHTmGY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/Zm9B-DvwOgw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Ru5fQZ714x8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>    <div style="break-before: page; page-break-before: always;"></div><h1 id="14-inspirations"><a class="header" href="#14-inspirations">14. Inspirations</a></h1>
<h2 id="chris-lattner"><a class="header" href="#chris-lattner">Chris Lattner</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Chris_Lattner">Chris Lattner</a> talks about working with Steve Jobs, Elon Musk and Jeff Dean:</p>
<ul>
<li>Being OK with not knowing now</li>
<li>Keys is not having right answer, but it is getting the right answer</li>
<li>If you ask a lot of dumb questions you get <strong>smarter</strong> really quick</li>
</ul>
<p>Chris is the main author of <a href="https://llvm.org/">LLVM</a> and related projects such as the <a href="https://clang.llvm.org/">Clang compiler</a> and the <a href="https://docs.swift.org/swift-book/">Swift</a> programming language</p>
<iframe width="720" height="480" src="https://www.youtube.com/embed/5siAT0jmw-Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="jeff-dean"><a class="header" href="#jeff-dean">Jeff Dean</a></h2>
<iframe width="720" height="480" src="https://www.youtube.com/embed/modXC5IWTJI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="15-datasets"><a class="header" href="#15-datasets">15. Datasets</a></h1>
<p>Common Crawl</p>
<p>The Common Crawl corpus contains <strong>petabytes of data collected since 2008</strong>. </p>
<p>It contains raw web page data, extracted metadata and text extractions.</p>
<p><a href="https://commoncrawl.org/the-data/get-started/">Common Crawl</a></p>
<h2 id="example-projects"><a class="header" href="#example-projects">Example Projects</a></h2>
<iframe width="720" height="480" src="https://www.youtube.com/embed/gOT7El8rMws" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="boston-housing-dataset"><a class="header" href="#boston-housing-dataset">Boston Housing Dataset</a></h1>
<ul>
<li><a href="https://www.cs.toronto.edu/%7Edelve/data/boston/bostonDetail.html">Boston House prices</a></li>
<li><a href="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/BostonHousing.csv">Boston House Price Dataset</a></li>
</ul>
<pre><code class="language-py">
import pandas as pd

url=&quot;https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/BostonHousing.csv&quot;
df = pd.read_csv(url)
df.head()

</code></pre>
<p><img src="datasets/img/1/boston-house-price-1.png" alt="Boston House Price" />
<img src="datasets/img/1/boston-house-price-2.png" alt="Boston House Price" /></p>
<p>There are 14 attributes (<strong>features</strong>) in each case of the dataset. They are:</p>
<pre><code>CRIM - per capita crime rate by town
ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
INDUS - proportion of non-retail business acres per town.
CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
NOX - nitric oxides concentration (parts per 10 million)
RM - average number of rooms per dwelling
AGE - proportion of owner-occupied units built prior to 1940
DIS - weighted distances to five Boston employment centres
RAD - index of accessibility to radial highways
TAX - full-value property-tax rate per $10,000
PTRATIO - pupil-teacher ratio by town
B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
LSTAT - % lower status of the population
MEDV - Median value of owner-occupied homes in $1000's

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="16-building-ml-for-industries"><a class="header" href="#16-building-ml-for-industries">16. Building ML for Industries</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lost-found-item-management"><a class="header" href="#lost-found-item-management">Lost-Found Item Management</a></h1>
<h2 id="problem-description"><a class="header" href="#problem-description">Problem Description</a></h2>
<p>Parcel delivery companies like UPS, FedEx needs a <strong>lost-and-found management</strong> solution:</p>
<p>There is possibility that item(s) packaged by the customers may fall out and becomes a candidate for the lost-and-found item</p>
<h2 id="solution"><a class="header" href="#solution">Solution</a></h2>
<p>We can build a ML and Deep Learning based Image detection and comparison system.</p>
<p><img src="industry/img/lost-found-item-mgmt-1.png" alt="lfm" /></p>
<h3 id="how-this-works"><a class="header" href="#how-this-works">How this works?</a></h3>
<ul>
<li>
<p>We have a database of the images which customers have reported that they have lost</p>
</li>
<li>
<p>At the storage and processing facility, we run those lost-and-found items on a conveyor</p>
</li>
<li>
<p>When the item reaches the Trigger point for scan image capture devices at Reader-L, Reader-R and Reader-T captures images at left, right and top. This may include bar-codes, UPC, QR-Code</p>
</li>
<li>
<p>These images are sent via WiFI to Image Collection, Composer and Processor (ICCP) device</p>
</li>
<li>
<p>ICCP makes use of Vision API and runs:</p>
<ul>
<li>
<p>Image Detection of the composed image (composed out of 3 images received for this item)</p>
</li>
<li>
<p>Image Compare to compare images stored in the Customer cases</p>
</li>
<li>
<p>Stores these attributes</p>
<ul>
<li>ScannedImageUUID</li>
<li>CustomerCaseImageUUID</li>
<li>MatchScore</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Based on the MatchScore we can detect/match the owner of this lost-and-found item</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="17hardware"><a class="header" href="#17hardware">17.Hardware</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="raspberry-pi"><a class="header" href="#raspberry-pi">Raspberry Pi</a></h1>
<p>The Raspberry Pi is a <strong>small computer</strong> that can do <strong>lots of things</strong>. </p>
<p>You plug it into a monitor and attach a keyboard and mouse.</p>
<p><img src="hardware/img/pi-plug-in.gif" alt="Ras Pi" /></p>
<p><img src="hardware/img/pi-labelled-names.png" alt="Ras Pi Labelled" /></p>
<h2 id="tensorflow-lite-on-pi"><a class="header" href="#tensorflow-lite-on-pi">Tensorflow Lite on Pi</a></h2>
<iframe width="720" height="460" src="https://www.youtube.com/embed/aimSGOAUI8Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="raspberry-pi-emulation-on-macos"><a class="header" href="#raspberry-pi-emulation-on-macos">Raspberry PI emulation on macOS</a></h2>
<pre><code># install info: https://github.com/faf0/macos-qemu-rpi
# https://joshondesign.com/2021/04/15/emu_pi_mac

pi@raspberrypi:~$ uname -a
Linux raspberrypi 5.4.51 #1 Sat Aug 8 23:28:32 +03 2020 armv6l GNU/Linux
pi@raspberrypi:~$ 


</code></pre>
<h2 id="resources-1"><a class="header" href="#resources-1">Resources</a></h2>
<ul>
<li>
<p><a href="https://projects.raspberrypi.org/en/projects/raspberry-pi-getting-started/1">Raspberry Pi Getting Started</a></p>
</li>
<li>
<p><a href="https://florianmuller.com/raspberry-development-environment-on-macosx-with-qemu">Raspberry Development Environment on MacOSX with QEMU</a></p>
</li>
<li>
<p>TensorFlow Lite</p>
<ul>
<li><a href="https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md">Part 2 - How to Run TensorFlow Lite Object Detection Models on the Raspberry Pi</a></li>
<li><a href="https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi">TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi</a></li>
</ul>
</li>
<li>
<p>Cell phone</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=jjX7nS3kIao">Make Your Own Raspberry Pi Cell Phone</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="20-tools"><a class="header" href="#20-tools">20. Tools</a></h1>
<ul>
<li><a href="tablesgenerator.com/markdown_tables">Markdown Table Generator </a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
