# BERT (Bidirectional Encoder Representations from Transformers

- bidirectional training of Transformer, a popular attention model, to language modelling.
- in contrast to (single-direction)  efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training.


<iframe width="800" height="500" src="https://www.youtube.com/embed/Kxpdlr6WJSY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## Neural architecture search (NAS)
-  is a technique for automating the design of artificial neural networks (ANN), a widely used model in the field of machine learning. 

## References

- [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
- [Building a Search Engine with BERT and TensorFlow](https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a)
- [(Pre-training BERT from scratch with cloud TPU](https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379)
- [Code:Pre-training BERT from scratch with cloud TPU](https://colab.research.google.com/drive/1nVn6AFpQSzXBt8_ywfx6XR8ZfQXlKGAz#scrollTo=vZPgpRl5g2e2)

