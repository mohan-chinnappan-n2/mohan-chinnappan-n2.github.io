
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Salesforce Data Integration Tips</title>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="https://mohan-chinnappan-n.github.io/css/navbar-blue.css">
<script src="https://mohan-chinnappan-n.github.io/sfdc/gs/js/split.js"></script>
<link rel="stylesheet" href="https://mohan-chinnappan-n.github.io/sfdc/gs/css/split.css">

<style> img { border-radius:20px; } </style>
</head>
<body>
<nav class="navbar navbar-default" role="navigation" style='margin-bottom:0px;'>
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="javascript:void(0)">Salesforce Data Integration Tips</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li class="active"><a href="#/home">Home</a></li>
      </ul>
      <ul class="nav navbar-nav">
        <li ><a href="/">Main</a></li>
      </ul>

    </div>
    <!-- /.navbar-collapse -->
  </nav>

    <div class="split split-horizontal" id="menu">
    <h4  class='slds-text-heading--medium'>Topics</h4> 
    <ol class="list-group">
        <li class="list-group-item"> <a href="#ldv">Large Data Volume</a> 
     <ol class="list-group">
         <li class="list-group-item"> <a href="#ldv_dl"> Data Loader - Bulk API</a> </li> 
          <li class="list-group-item"> <a href="#ldv_dx"> SFDX plugin - Bulk API</a> </li> 
          <li class="list-group-item"> <a href="#ba1">Bulk API v1</a> </li> 
          <li class="list-group-item"> <a href="#ba2">Bulk  API v2</a> </li> 
      </ol>
         <li class="list-group-item"> <a href="#eb">Event Based Data Sync</a> </li> 
         <li class="list-group-item"> <a href="#ref">References</a> </li> 


      </li> 
    </ol>
 
   </div>
    <div class="split split-horizontal" id="content">

       <div id="ldv">
        <h4  class=''>Large Data Volume</h4> 
            <ul class="list-group">
               <li class="list-group-item"> 
         <h4>Using Bulk API  </h4>

<img src="img/bulk-api-1.png" alt="">
<p>
The Bulk API  developed specifically to simplify the process of uploading large amounts of data. It is optimized for inserting, updating, upserting, and deleting large numbers of records <b>asynchronously</b> by submitting them in batches to Force.com, to be processed in the <b>background</b>.
</p>
<p>
Uploaded records are streamed to Force.com to create a new job. As the data rolls in for the job it is stored in temporary storage and then sliced up into user-defined batches. Even while your data is still being sent to the server, the Force.com platform submits the batches for processing.
<br/>
Batch sizes should be adjusted based on processing times. Start with 5000 records and adjust the batch size based on processing time. If it takes more than five minutes to process a batch, it may be beneficial to reduce the batch size. If it takes a few seconds, the batch size should be increased. If you get a timeout error when processing a batch, split your batch into smaller batches, and try again.
</p>
<p>
Batches can be processed in parallel or serially depending upon your needs. The Bulk API moves the functionality and work from your client application to the server. The API logs the status of each job and tries to reprocess failed records for you automatically. 
</p>
<p>
<hr/>
Salesforce provides an additional API, <b>Bulk API 2.0</b>, which uses the REST API framework to provide similar capabilities to Bulk API. Use Bulk API 2.0 instead of Bulk API if you want a simplified process for inserting, updating, upserting, or deleting large sets of data. Bulk API 2.0 does not currently support query or queryAll.<br/>

Bulk API v2 does away with the need to manually break up data into batches. Simply submit jobs with the full set of records, and Salesforce automatically determines the most efficient way to batch the data.

<br/>

Bulk API v2 simplifies the basic daily limits. Instead of having limits based on the number of Bulk jobs and batches, you’re simply limited to a maximum number of records <b>(100 million) per 24 hour period</b>.

</p>
                  <ul class='list-group'>
                      <li class='list-group-item'>
<ol class="list-group">
<li class="list-group-item">Create a new <b>job</b> that specifies the object and action.</li>
<li class="list-group-item">Send data to the server in a number of <b>batches</b>.</li>
<li class="list-group-item">Once all data has been submitted, close the <b>job</b>. Once closed, no more batches can be sent as part of the job.</li>
<li class="list-group-item">Check status of all batches at a reasonable interval. Each status check returns the state of each batch.</li>
<li class="list-group-item">When all batches have either completed or failed, retrieve the result for each batch.</li>
<li class="list-group-item">Match the result sets with the original data set to determine which records failed and succeeded, and take appropriate action</li>
<li class="list-group-item">At any point in this process, you can abort the job. Aborting a job has the effect of preventing any <b>unprocessed batches from being processed. It doesn't undo the effects of batches already processed.  </b></li>
<li class="list-group-item">
To configure Data Loader to use the Bulk API for inserting, updating, upserting, deleting, and hard deleting records: <br/>
Open the Data Loader.  Choose Settings | Settings.  Select the Use Bulk API option.  Click OK.

</li>

</ol>

<pre>
<b>Folder content</b>

$<b> tree</b>
.
├── error022019081012672.csv
├── fieldMap.sdl
├── harvest_field-2.csv
├── harvest_field.csv
└── success022019081012672.csv

0 directories, 5 files

$ <b> cat fieldMap.sdl</b>
#Mapping values
#Wed Feb 20 20:09:30 EST 2019
Name=Name
Asset_Under_Management__c=Asset_Under_Management__c
Status__c=Status__c
</pre>

<h4 id='ldv_dl'>Using Data Loader Configured to use Bulk API</h4>
 <img src="img/dl-0.png" width='700' alt=""> <hr/>
 <img src="img/dl-1.png" width='700' alt=""> <hr/>
 <img src="img/dl-2.png" width='700' alt=""> <hr/>
 <img src="img/dl-3.png" width='700' alt=""> <hr/>
 <img src="img/dl-4.png" width='700' alt=""> <hr/>
 <img src="img/dl-5.png" width='700' alt=""> <hr/>
 <img src="img/dl-6.png" width='1000' alt=""> <hr/>



<pre>
$ cat <b>success022019081012672.csv</b>
"ID","NAME","STATUS__C","ASSET_UNDER_MANAGEMENT__C","STATUS"
"a00B0000009skwAIAQ","Pine East Field","Normal","310000","Item Created"
"a00B0000009skwBIAQ","Pine West Field","Normal","312000","Item Created"
"a00B0000009skwCIAQ","Pine North Field","Normal","313000","Item Created"
"a00B0000009skwDIAQ","Pine South Field","Normal","315000","Item Created"

$ cat <b>error022019081012672.csv</b>
"NAME","STATUS__C","ASSET_UNDER_MANAGEMENT__C","ERROR"
</pre>





                    </li>
                  </ul>


<h4 id='ba1'>Bulk API v1</h4>
<pre>
1. Get an authenticated session ID, likely via a completely different API, such as the SOAP API.
2. Create a Bulk API v1 job.
3. Break up job data into batches. This can be a complex task and in many scenarios will involve:
  - Break up data to fit within the Bulk API v1 batch size limit for records, and batch size limit for total size for a batch.
  - Decide if you need to use special processing headers, like compression or PK Chunking.
  - Analyze data chunks for potential locking issues due to data skew, which could result in very slow or failed bulk processing. Re-organize batches as needed.
  - Minimize the amount of data post-processing actions (like triggers and Workflow rules) that might result in batch processing timeouts.
  - After all of this, if it turns out your batches take too long to process, go through the process of determining the best way to organize your batches all over again.
4. Upload data in batches.
5. Verify the batches uploaded properly.
6. Close the job, which tells Salesforce to start processing the records.
7. Check the status of the job.
8. If the job completes with no errors, we’re done.
9. If the job completes but encountered errors during processing, iterate over each batch in the job, collect 
results of successful and failed records for the batch, determine why the records failed, and re-assemble the data to submit a new job as needed.



</pre>
<h4 id='ba2'>Bulk API v2</h4>
<pre>
1. Authenticate using OAuth.
2. Create a Bulk API v2 job.
3. Upload all your data.
4. Close the job, which tells Salesforce to start processing the data.
5. Check the status of the job.
6. If the job completes with no errors, we’re done.
7. If the job completes but encountered errors during processing, request the complete list of failed 
records with one API call, determine why the records failed, and submit a new job as needed.

<hr>
<b>1. Authenticate using OAuth</b>
 - Issue requests to <b>https://login.salesforce.com/services/oauth2/authorize</b> to obtain an authentication token.

<b>2. Create a job</b>
- Create a new Bulk API v2 job by issuing a <b>POST</b> request to <b> /services/data/v44.0/jobs/ingest/</b> with the following request body:
<b>{  "object" : "Contact", "contentType" : "CSV", "operation" : "insert" }</b>
This creates a job that will insert new Contact records.

<b>3. Upload the data</b>
 - Issue a <b>PUT</b> request using the JOB ID returned from the previous request to the following URI:
<b>/services/data/v44.0/jobs/ingest/JOB ID/batches/</b>
The request body will be CSV data of all the records you want to upload (with the Content-Type request header set to text/csv).

<b>4. Close the job</b>
Issue a <b>PATCH</b> request again using the JOB ID, to the following URI: <b>/services/data/v44.0/jobs/ingest/JOB ID/</b>
With the following request body:  <b>{  "state" : "UploadComplete"  } </b>
This tells Salesforce we’re done uploading data for the job, and Salesforce will start inserting the records.

<b>5. Check the status of the job</b>
Issue a <b>GET</b> request to: <b>/services/data/v44.0/jobs/ingest/JOB ID/</b>
Look for a job state of JobComplete to know Salesforce is done processing the job.

<b>6. Get errors for any failed records</b> 
If the job status indicates that some records encountered errors during processing, 
issue a <b>GET</b> request to the following URI to get a full list of the failed records:
<b>/services/data/v44.0/jobs/ingest/JOB ID/failedResults/</b>
</pre>
<h4>Start Job</h4>
<pre>
# $1: jobstart.json
export RESOURCE='/services/data/v41.0/jobs/ingest/'
     curl -X POST $URL/$RESOURCE  -H "Authorization: Bearer $AT " -H "Content-Type: application/json" -d "@$1" | jq
</pre>
<img src="img/wb-startjob.png" width='1000' alt="">
<h4>Load Data</h4>
<pre>
# $1: JOBID, $2:data.csv
export RESOURCE=/services/data/v41.0/jobs/ingest/$1/batches
     curl -i -X PUT $URL/$RESOURCE  -H "Authorization: Bearer $AT " -H "Content-Type: text/csv" -d "@$2" 
</pre>
<img src="img/wb-dataload.png" width='1000' alt="">
<h4>Close Job</h4>
<pre>
# $1: JOBID,
export RESOURCE=/services/data/v41.0/jobs/ingest/$1/
     curl -X PATCH $URL/$RESOURCE  -H "Authorization: Bearer $AT " -H "Content-Type: application/json" -d "@jobdone.json" 
#
cat jobdone.json 
 {  "state" : "UploadComplete"  } 

</pre>
<img src="img/wb-jobcomplete.png" width='1000' alt="">
<h4>Check Status</h4>
<pre>
export RESOURCE=/services/data/v41.0/jobs/ingest/$1/
     curl -i -X GET $URL/$RESOURCE  -H "Authorization: Bearer $AT " -H "Content-Type: application/json"  

</pre>
<img src="img/wb-loaded.png" width='1000' alt="">
<h4>Data Loaded</h4>
<img src="img/wb-dataloaded.png" width='1000' alt="">

<hr/>
<a href="https://trailhead.salesforce.com/en/content/learn/modules/api_basics/api_basics_bulk">Trailhead: Use Bulk API</a>
<hr/>
<h4 id='ldv_dx'>Using SFDX Plugin to use Bulk API</h4>
 <img src="img/di-dx-2.png" width='1000' alt=""> <hr/>
 <img src="img/di-dx-1.png" width='1000' alt=""> <hr/>
 
<h4 id='eb'>Event Based Data Sync</h4>
<p> Well suited for near-real time data sync with external systems </p>
<hr/>
 <img src="https://mohan-chinnappan-n.github.io/sfdc/img/pe/pe-1.png" width='1000' alt=""> <hr/>
 <img src="https://mohan-chinnappan-n.github.io/sfdc/img/pe/pe-2.png" width='1000' alt=""> <hr/>
 <img src="img/pe-3.png" width='1000' alt=""> <hr/>


<h4 id='ref'>References</h4>
<a   target='_new' href="https://mohan-chinnappan-n.github.io/sfdc/pevents.html#/home"> Platform Events</a>
<br/>
<a  target='_new' href="https://mohan-chinnappan-n2.github.io/2019/sqlmed/fd.html">Foreign Data Wrapper (FDW) and SQL/MED</a>
 <br/>
<a  target='_new' href="https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/asynch_api_batches_intro.htm">Bulk API Developer Guide
</a>
  <br/>
<a  target='_new' href="https://developer.salesforce.com/blogs/2017/12/slim-new-bulk-api-v2.html">Comparing Bulk API v1 and v2 </a>
   <br/>
<a  target='_new' href="https://developer.salesforce.com/docs/atlas.en-us.api.meta/api/best_practices_with_any_data_loader.htm">Best Practices with Any Data Loader
</a>
    <br/>
<a  target='_new' href="https://help.salesforce.com/articleView?id=000004601&type=1">KB: Best practices when you migrate data </a>
     <br/>
<a  target='_new' href="https://help.salesforce.com/articleView?id=import_which_data_import_tool.htm&type=5">Choosing a Method for Importing Data
</a>
      <br/>
<a  target='_new' href="https://developer.salesforce.com/docs/atlas.en-us.financial_services_cloud_admin_guide.meta/financial_services_cloud_admin_guide/fsc_admin_data_loader_upload_sequence.htm">FSC: Load Data </a>
 
 </li>
           </ul>
      </div>




   </div>

<script>
    Split(['#menu', '#content'],  {sizes: [15, 85]});
</script>
</body>
</html>

