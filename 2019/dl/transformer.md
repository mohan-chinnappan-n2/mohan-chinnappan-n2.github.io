## Transformer
- Transformer model handles variable-sized input using **stacks of self-attention layers** instead of RNNs or CNNs. 
- This general architecture has a number of advantages:
